[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About DS Essentials",
    "section": "",
    "text": "About DS Essentials\nDigital Scholarship & Data Science Essentials for Library Professionals (DS Essentials) is an open and collaboratively curated training reference resource. It aims to make it easier for LIBER library professionals to gain a concise overview of the new technologies that underpin digital scholarship and data science practice in research libraries today, and find trusted training materials recommendations to start their professional learning journey. The project began in late 2023 as a joint collaboration between the Digital Scholarship & Digital Cultural Heritage (DSDCH) and the Data Science in Libraries (DSLib) working groups of LIBER. Work is currently underway to create a first edition ready for the LIBER Winter Event 2024. Contributions to this work are very much welcome!",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "index.html#our-aims",
    "href": "index.html#our-aims",
    "title": "About DS Essentials",
    "section": "Our Aims",
    "text": "Our Aims\n\nPresent a central destination for newcomers to become more familiar and conversant with the foundational concepts, methods and tools of digital scholarship and data science practice in libraries. Examplar topics include:\nHelp library staff gain a better understanding of digital scholarship and data science practice in libraries by providing contextualised and gentle introductions to key topics through guides, self-guided training materials, project case studies, jargon busting and recommended reading lists\nEnable staff, in LIBER membership and beyond, to explore and immediately access a wealth of self and group study educational resources, available to them through a curated catalogue with faceted search and discovery\nFoster a culture of continuous learning and professional development and facilitate the development of localised training opportunities in LIBER institutions by providing guidance and training materials suitable for self and group study\nOffer a centralised, curated and trusted home for educators to contribute, and have reused, their high-quality training materials\nRaise awareness of local, national and international networks, and associated training events, relevant to delivering state of the art services in libraries\nFurnish current reports and competency skills frameworks pertaining to digital scholarship and data science in libraries",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "index.html#who-is-it-for",
    "href": "index.html#who-is-it-for",
    "title": "About DS Essentials",
    "section": "Who is it for?",
    "text": "Who is it for?\nAre you someone working in or around research libraries with an interest in learning more about how to do cool and interesting things with digital collections and data at your institution? Wondering how data science techniques can help you in your work? Are you interested in gaining valuable digital literacy skills and knowledge to support emerging areas of modern scholarship such as Digital Humanities? Do you need some of the technology jargon you hear about these days demystified?\nThen this resource is for you!\nIt is very important to us that this resource is inclusive and intellectually accessible, challenging but not terrifying and as such we focus primarily on an introductory audience where no programming or particular digital skills are required.\nThough written primarily from the research library professional perspective these guides will be useful for anyone currently (or aspiring to be) working in and around digital collections and data in heritage institutions:\n\nLibrary & Information Science students\nProject managers\nDevelopers\nInformation specialists\nMetadata Managers\nSubject librarians\nSystem librarians\nInstitutional leadership\n\nAnd so many more!",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "index.html#why-this-resource",
    "href": "index.html#why-this-resource",
    "title": "About DS Essentials",
    "section": "Why this resource?",
    "text": "Why this resource?\nOver the past few decades, the development of excellent self-paced tutorials and training materials relating to undertaking digital scholarship and data science in libraries have proliferated online.\nFor library professionals who are relatively new to this area however, it can be hard to know where to begin! Without knowing a little bit about the context of how new technologies are being deployed in research libraries, this dearth of resources can seem quite daunting. Even if you might have an idea of what learning you’d like to undertake, it can be difficult and time consuming to try and navigate the wealth of individual training resources out there on your own.\nDigital Scholarship and Data Science Essentials for Library Professionals is a training reference resource we have developed to remove some of the barrier of having to hunt for recommended training and resources to get you on the learning ladder.\nFrom working with data, data management, digital storytelling, handwritten recognition technologies and more, our Topic Guides provide contextualised and gentle introductions, written from the library practitioners perspective, to a wide range of key topics and technologies relevant to working innovatively with our digital collections and data.",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "index.html#what-are-topic-guides",
    "href": "index.html#what-are-topic-guides",
    "title": "About DS Essentials",
    "section": "What are Topic Guides?",
    "text": "What are Topic Guides?\nTopic Guides are the heart and soul of Digital Scholarship and Data Science Essentials for Library Professionals. The aim of each is to provide research library professionals with a gentle and concise introduction to the key topics in this area today. They offer those with no prior knowledge, quick and curated guidance to personally recommended hands-on tutorials, use-cases, articles, videos, networks and communities of practice to deepen learning.\nEach Topic Guide follows a fixed structure, consisting of five components in order to make it easier to quickly find what you might need:\n\nIntroduction to the topic\nRelevance to the library sector (Case studies/Use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended Reading & Viewing\nTaking the next steps (Communities of Practice)",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "About DS Essentials",
    "section": "How to use this resource",
    "text": "How to use this resource\nThis resource has a dual purpose; it is written as a tool for self-study but also as a guide for individuals and institutions interested in establishing their own local training programmes.\nAs a self-study resource, we hope it can serve as a useful first point of entry into complex topics in digital scholarship and data science and their application in the library world. It is not meant to be completed in any particular order, rather, learners are invited to jump in and out of individual topic guides as personal curiosity or practical need dictates.\nThe materials here can also be utilised in group study, as part of a reading group or even a hands-on Hack & Yack. See our special section on “How to start your own training programme” for some tips and tricks from British Library Digital Research Team colleagues on utilising the materials referenced here at your own library or within your network.",
    "crumbs": [
      "**ABOUT**",
      "About DS Essentials"
    ]
  },
  {
    "objectID": "project-overview.html",
    "href": "project-overview.html",
    "title": "Project Background",
    "section": "",
    "text": "Core Project Team\nThe site and content are maintained by the Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nNeha Moopen, Site Maintainer DSLib\nPeter Verhaar, Editor DSLib",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "project-overview.html#community-contributors",
    "href": "project-overview.html#community-contributors",
    "title": "Project Background",
    "section": "Community Contributors",
    "text": "Community Contributors\nEach Topic Guide is written by specific named contributors but we also welcome changes and contributions to this resource via logging issues or if you’re a more seasoned GitHub user, via pull requests to the github repository. A full list of contributors will be compiled here once the first edition is complete.",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "project-overview.html#acknowledgements",
    "href": "project-overview.html#acknowledgements",
    "title": "Project Background",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe project team would like to thank all of our LIBER working group members who have contributed to this resource and supported our endeavours here. We have taken great inspiration from other incredible training initiatives such as the British Library Digital Scholarship Training Programme, DH Literacy Guidebook and The Programming Historian in the development of this resource and thank those projects for paving the way!",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contribute to Our Project",
    "section": "",
    "text": "Ways to contribute to this project",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#ways-to-contribute-to-this-project",
    "href": "contributing.html#ways-to-contribute-to-this-project",
    "title": "Contribute to Our Project",
    "section": "",
    "text": "Contribute a new Topic Guide (individually or in collaboration with others). Choose from our existing wishlist or propose a new one. Join an upcoming writing sprint or get in touch with us to discuss digitalresearch@bl.uk your idea!\nSuggest edits to an existing Topic Guide or Additional Resources by opening an Issue or adding to existing ones on the project Github.\nJoin one of the LIBER Working Groups Digital Scholarship and Digital Cultural Heritage WG or Data Science in Libraries (open to all staff of LIBER member institutions) to keep up with the developments and ongoing maintenance of this project!",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#join-a-writing-sprint",
    "href": "contributing.html#join-a-writing-sprint",
    "title": "Contribute to Our Project",
    "section": "Join a Writing Sprint!",
    "text": "Join a Writing Sprint!\n\nOur first Topic Guide Sprint was held online Tuesday 14 May 2024. Our next one will be ONLINE Tuesday 03 June 2024 9:00-11:00 (BST): Register Today!\n\nIf you would like to take part please Register Today to be sent the meeting calendar invite, sprint guidance, login details and access to your Topic Guide Template in case you want to prep/get started early. Feel free to contact digitalresearch@bl.uk with any questions!\n\nPreparing for the sprint:\n\nPlease have a quick read through the Welcome and Topic Guide pages to familiarise yourself with this resource and its purpose.\nWhen you Register you’ll see a list of our latest Topic Guides on our wishlist. Please check off any and all Topic Guides you would be happy to work on during the day of the sprint. On the day each attendee will focus on writing just one Topic Guide however. The information you provide before the sprint will help us organise this ahead of time so that as many Topic Guides as possible have at least one dedicated author.\n\nPlease have a quick read through the Author Guidance and Style Guide section.\nWe’ll be in touch after we receive your registration to send you more details and useful information ahead of the sprint so look out for our email! If you are sent a copy of the Topic Guide Template (Google Doc) that corresponds to your topic of choice ahead of time, add your name to the header, and feel free to add notes or even make a start on drafting your guide before the sprint day if you’re keen!\n\n\n\nWhat to expect on the day:\nA two hour writing sprint will generally follow this format:\n\nWelcome and participant introductions (10 minutes)\n\nOverview of the project & explanation of how the day will run (15 minutes)\n\nAllocation and confirmation of individual Topic Guide Authorship for the day (15 minutes): As a group we’ll go over the wishlist and all participants will be given access to their particular Topic Guide google doc template they’ll be using during and after the sprint.\nWriting Phase (60 minutes): You’ll have an hour to begin writing your respective guide in your Google Doc template. If more than one participant are working together on a topic we can provide breakout rooms so that you can discuss amongst yourselves how to break up the work, and share ideas as you go along. You can have your camera/mic on or off during this time and co-chairs will be on hand to answer any questions or give advice on topics.\nWrapping Up & Logging progress through Github Issues (15 minutes): We have provided an Issue Template which is pre-formatted as a Topic Guide submission checklist and will walk participants through the process of opening a new issue for your draft submission. Please fill in the issue template and provide the necessary information relating to where your draft submission is currently at. Once the issue is saved, it can continue to serve as a space to continue discussion of the ongoing status of your submission and log updates where necessary, including notifying us of when it is fully ready for final review by maintainers.\nClosing Remarks & Next Steps (5 minutes)\n\n\n\nAfter the sprint:\nDuring the sprint, you’ve drafted your Topic Guide in a Google Doc. This Google Doc can continue to serve as a ‘living document’ following the sprint until you feel the content is ready to be published. Throughout this process, we will use your Github issue to maintain an overview of the docs in terms of their status and action points so please keep this updated. Each Topic Guide will be allocated a specific Maintainer contact (one of the co-chairs) who will be in touch and work with you to see the process through to completion.\nThings to note when creating and working with the Issue Template for your newly created Topic Guide:\n\nIf you want to include resources such as images and videos etc. which cannot be linked to via the web you might have to provide the files themselves to us. You can attach any local files as a comment on the Issue you have created for your Topic Guide.\nYou may request placing your Google Doc under restricted access if you wish. We will not do this by default, but we respect that some contributors would like to keep drafts private until fully ready for submission.\n\nWhen your Topic Guide is ready for our review in Google Docs, please let us know by logging an update over on Github as a comment on the Issue stating this. You can also tag the maintainers so they receive a notification as well. Maintainers will generally work with you on the final edits necessary for the submission through track changes and comments within the Google Doc itself. Once these have all been resolved maintainers will log a status update indicating “Review complete; Final Submission Ready; Make live” in the GitHub issue.",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#ongoing-maintenance-and-review",
    "href": "contributing.html#ongoing-maintenance-and-review",
    "title": "Contribute to Our Project",
    "section": "Ongoing Maintenance and Review",
    "text": "Ongoing Maintenance and Review\nWe view this resource as capturing a snapshot in time and will endeavour to fully review all content a minimum of once annually to check for link rot, and content relevancy. The project team editors are committed to make quick fixes when raised throughout the year, while formal sprints and reviews coinciding with LIBER Annual Summer Conference and Winter events will be used for soliciting new content and undertaking more complex updates to the resource each year and creating new editions as necessary.\nJoin one of the LIBER Working Groups Digital Scholarship and Digital Cultural Heritage WG or Data Science in Libraries (open to all staff of LIBER member institutions) to keep up with the developments and ongoing maintenance of this project!",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "guidelines.html",
    "href": "guidelines.html",
    "title": "Author Guidance and Style Guide",
    "section": "",
    "text": "Structuring the Topic Guide\nEach Topic Guide follows a distinct structure, beginning with a Header Section composing of a title, contributor information, published date, last modified info, and suggested citation followed by five key content blocks detailed below. Please have a look at the IIIF Guide as an example of how these pieces all fit together into one complete Topic Guide.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "guidelines.html#structuring-the-topic-guide",
    "href": "guidelines.html#structuring-the-topic-guide",
    "title": "Author Guidance and Style Guide",
    "section": "",
    "text": "Header Section\n\nTopic Guide Title: IIIF\nContributor(s) Name and (Orcid ID): Nora McGregor (ORCID iD)\nPublished date: 27/03/2024\nSuggested Citation: Nora McGregor, “IIIF,” Digital Scholarship & Data Science Essentials for Library Professionals (2024), [DOI link]\n\nNote: The project team will ensure each Topic Guide is given an individual DOI either through British Library Research Repository or Zenodo.\n\n\nI: Introduction to the topic (Approximately 500-700 words)\nThis introduction section should be pitched at a beginner/foundational level and gives a concise overview of the topic. It should be written in a relaxed and natural way. This section does not need to contain the world’s knowledge, just enough high-level knowledge to get the key concepts across.\n\nImagine that a colleague has come to you casually asking about the topic over tea. How might you go about explaining it to them in your own words? During the course of that casual conversation what key things would you leave in and what might you leave out in the interest of getting them to a basic understanding quickly?\n\nLinking can be used liberally for jargon busting throughout if explaining a particular term is more complex than our word count allows. Please see the Style Guidelines below for more writing tips!\n\n\nII: Relevance to the Library Sector (Approximately 300-500 words)\nThis section provides a clear explanation of the topics’ specific relevance to the work of libraries.\nIt should contain:\n\nA short paragraph or two setting the scene as to why the topic is relevant to the work of libraries. Here you might also like to present opportunities and, if relevant, potential challenges for libraries around the topic as well.\nUp to 3 examples of real world (or potential) applications/case studies/projects briefly explained in a 100 max word summary provided each with links to further information if available. Note that it is not necessary to write up or create a new case study yourself here! Rather, we’re looking for briefly summarised references to existing ones. If there are quite a few other examples the contributor would like to reference include, links to these can be added at the end of this section (Example: “For further case studies, visit….”).\n\n\n\nIII: Hands-on activity and other self-guided tutorial(s)\nThe objective of this section of the topic guide is to enable learners to familiarise themselves with the basics of the topic through active practice via self-paced tutorials and hand-on activities. Authors are not expected to create new activities or tutorials in this section, but rather to provide selected links to existing hands on tutorials which are known by them to have proven value and can be personally recommended for library professionals in particular.\nThe tutorials recommended should be free, online and suitable for independent study, and ideally, focussed on the library professional perspective where possible. Authors may wish to link to practical exercises or quizzes, specific online lessons that may exist in other online platforms, Juptyer Notebooks and GLAM workbench materials that provide detailed explanations of the steps learners can follow.\nFor the sake of consistency across the various topic guides, it is helpful if tutorial references are structured as follows:\n\nThe title/name of the activity/tutorial, the URL (added as a hyperlink), and citation information (please use APA Style and/or provide a DOI if there is one)\nA brief, personal explanation below it (no more than 200 words) as to why the author recommends this particular tutorial, and, optionally, an indication of topics covered and the level of complexity.\n\n\n\nIV: Recommended Reading & Viewing (Approximately 200-400 words)\nThe section on recommended reading and viewing contains references to more passive learning resources such as:\n\nOpen access articles discussing the topic at a general level, or containing contextual information.\nVideo recordings of lectures about the topic, which do not demand practical activities from the viewer Podcasts about the topic\n\nWhen including citations please make sure to either write them in APA Style and/or simply provide the DOI with your text so that the project team can compile a dedicated Zotero Library.\n\n\nV: Taking the next steps (Approximately 200-400 words)\nThis section provides guidance to library professionals on where to take their learning journey further. It should include:\n\nWhere to find (local/national/international) Communities of Practice or other relevant networks and organisations who can help with furthering their understanding of the topic.\nIf relevant you might also point to specific summer school courses, conferences and events that may further enhance learning.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "guidelines.html#style-guidelines",
    "href": "guidelines.html#style-guidelines",
    "title": "Author Guidance and Style Guide",
    "section": "Style Guidelines",
    "text": "Style Guidelines\n\nWriting Accessible, Natural, and Internationally Inclusive Content\nTo ensure consistency and inclusivity across our content, please consider these general guidelines:\n\nClarity and Simplicity: Write in a clear and straightforward manner, using simple language that is easy to understand for learners of all backgrounds and proficiency levels.\nLinking to Technical Terms: When introducing technical terms or concepts that may be unfamiliar to some learners, provide hyperlinks to additional resources or definitions where they can learn more. This helps to enhance understanding and allows learners to explore topics in more depth at their own pace. Ensure that the linked resources are reliable and authoritative to provide accurate information to the learners.\nAvoid Colloquialisms and Regionalisms: While it’s essential to maintain a casual and natural tone, please refrain from using colloquial expressions or regionalisms that may not be universally understood by our diverse audience.\nCultural Sensitivity: Be mindful of cultural differences and avoid language or examples that may be offensive or insensitive to any group of people. When providing examples or references, strive for universality and inclusivity.\nGender Neutrality: Use gender-neutral language whenever possible to ensure inclusivity and avoid assumptions about gender roles or identities.\nGlobal Perspective: Consider the international nature of our audience when crafting examples, scenarios, and references. Aim for content that resonates with learners from various cultural backgrounds and geographical locations.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "licensing.html",
    "href": "licensing.html",
    "title": "Licensing & Re-use",
    "section": "",
    "text": "Citing this Resource\nSuggested citations are written in APA (American Psychological Association) citation style for Journal Articles and are given on each individual guide:\nMcGregor, Nora, “IIIF,” Digital Scholarship & Data Science Essentials for Library Professionals (2024), [DOI link]\nTo cite the full resource:\nMcGregor, N., Verhaar, P., Moopen, N., Double, J., Irollo, A., & Kiraly, P. (Eds.). (2024, March 12). Digital Scholarship & Data Science Essentials for Library Professionals. https://libereurope.github.io/ds-essentials",
    "crumbs": [
      "**ABOUT**",
      "Licensing & Re-use"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "If you’d like to ask a question of the project team you can please send an email to Nora McGregor, nora.mcgregor@bl.uk or any of Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nNeha Moopen, Site Maintainer DSLib\nPeter Verhaar, Editor DSLib",
    "crumbs": [
      "**ABOUT**",
      "Contact"
    ]
  },
  {
    "objectID": "topicguides.html",
    "href": "topicguides.html",
    "title": "TOPIC GUIDES",
    "section": "",
    "text": "Topic Guides are the heart and soul of Digital Scholarship and Data Science Essentials for Library Professionals. The aim of each is to provide research library professionals with a gentle and concise introduction to the key topics in this area today. They offer those with no prior knowledge, quick and curated guidance to personally recommended hands-on tutorials, use-cases, articles, videos, networks and communities of practice to deepen learning.\nEach Topic Guide follows a fixed structure, consisting of a descriptive header and five key components in order to make it easier to quickly find the information you might need:\n\nIntroduction to the topic\nRelevance to the library sector (case studies/use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended reading & viewing\nTaking the next steps (finding Communities of Practice)\n\nThe guides are not presented, nor meant to be completed, in any particular order, rather, you are invited to jump in and out of individual topics as personal curiosity or practical need dictates.\nThe Topic Guide list in this first edition draws heavily on the existing skills framework and topics covered in the British Library’s Digital Scholarship Training Programme, as well as those recommended by attendees of a series of development workshops held with LIBER working groups throughout 2023.\nOur current wishlist includes but is not limited to topics such as:\n\n2D/3D Imaging: 3D Modelling and RTI\nAutomatic Text Transcription (OCR/HTR)\nDemystifying Computational Environments for Digital Scholarship\nClimate Change and Sustainability\nCollections as Data\nCopyright and Licensing\nCrowdsourcing & Citizen Science in the cultural heritage context\nComputer Vision\nCultural Competency & Ethics\nData Visualisation\nDigital Mapping\nDigital Scholarship Project Management, Evaluation and Assessment\nDigital Storytelling\nGetting started in programming\nGithub and GitPages/Quarto\nIIIF\nLinked Open Data\nMachine Learning and AI in Libraries Literacies\nOpen Research\nResearch Data Management\nWhat is an API?\nWikimedia\nWorking with Data",
    "crumbs": [
      "**TOPIC GUIDES**"
    ]
  },
  {
    "objectID": "ml-ai.html",
    "href": "ml-ai.html",
    "title": "AI & ML in Libraries Literacies",
    "section": "",
    "text": "Introduction: AI & ML terms demystified\nAI is mentioned absolutely everywhere these days, first it was just in movies, the news, but now it’s cropping up in our library meetings and strategies and funding calls, but what does it really mean, particularly in a library context? Let’s try to get to the bottom of this!\nTo do that I always like to start off with a bit of basic jargon busting.\nArtificial Intelligence (AI) is actually a really broad field of computer science (and an umbrella term) that refers to the research and development of systems and machines capable of doing tasks that typically require human intelligence to perform, such as\nSometimes folks may speak of or refer to AI as systems and machines that actually have true intelligence, and though today’s AI systems are shockingly convincing in how well they perform, what we’re seeing today are just very advanced machine learning algorithms and models performing specific and discrete functions extremely well! We’re a long way off (if ever) from machines having sentience (or, Generalised Artificial Intelligence (GAI)/Strong AI) so don’t worry!\nYou might also sometimes hear people talk about Traditional AI vs Generative AI. Traditional AI refers to using machine learning based systems for doing tasks like classifying data (e.g., assigning labels to images, automatically transcribing handwritten texts, or identifying genre of digitised texts). This is the type of AI we make a whole lot of use of in the library world. Generative AI on the other hand refers broadly to systems whose primary function is to generate new content (e.g., conversation, books, art). This is where conversation generating AI systems like ChatGPT (Generative Pre-trained Transformer) fall under for example and we’re only just now exploring the potential applications for these new powerful Generative AI systems in library work.\nWhenever AI is being discussed you may often hear the term Machine Learning (ML) mentioned, and sometimes they’re used interchangeably which can be confusing!\nMachine Learning (ML) is more specifically a main subfield of AI and core technology underpinning the other subfields of AI, that focuses on the development of algorithms and models that allow computers to learn patterns and relationships from data and make predictions on new data. Instead of being explicitly programmed for specific tasks, ML algorithms use data to learn and improve their performance over time.\nAn algorithm is a plan, a set of step-by-step instructions, in order of operation, for solving a problem or performing a task. Making a sandwich is a classic example. “Get two pieces of bread…” If we want to tell a computer to do something, we have to write a computer program that will tell it, step-by-step, exactly what we want it to do and how we want it to do it.\nA machine learning algorithm is a special kind of algorithm designed to help computers learn from data. Instead of giving the computer explicit instructions for every task, we give it data and let it find patterns, relationships, and trends in that data in order to make decisions or predictions on new data on its own.\n“Training a model” is the process of teaching a machine learning algorithm to make predictions or decisions based on data. It’s important to remember that data is the lifeblood of ML and the model is only as good as the type and quality of data you give it.\nA machine learning model represents what was learned by a machine learning algorithm and this is what can be used to make predictions on new data without needing explicit instructions. The model contains the rules, numbers, and any other algorithm-specific data structures required to make predictions on new data. If it doesn’t work very well, you can go back and give the algorithm more data or tweak its parameters to create a new better model.\nWe actually do quite alot of the above at the British Library, you can read more about the actual process and results here, and I’ll cover a bit more of this in the next section!\nSnapshot of an interface from Transkribus software used to create data to train a model to recognise handwritten arabic manuscript pages from a British Library collection.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#introduction-ai-ml-terms-demystified",
    "href": "ml-ai.html#introduction-ai-ml-terms-demystified",
    "title": "AI & ML in Libraries Literacies",
    "section": "",
    "text": "Reasoning\nProblem-solving\nLearning\nPerception\n\n\n\n\n\n\nThe primary task of Machine Learning is prediction.\n\n\n\n\n\n\nHere’s a super simplistic example of this type of process in action:  Let’s say I have thousands of images of handwritten manuscript pages digitised from our library collection and ready to go online. The problem is, I want to make them all text searchable as well, but to transcribe these all by hand would take me well into my retirement and I have other things to be getting on with. I would like to train a machine learning model to help me automatically recognise the handwritten text in this collection. I would first need to somehow show my machine learning algorithm examples of a correct result so that it could start to recognise the pattern of that (in this case, text on a page). To do this I can show it images with associated text annotations which have been transcribed perfectly by me, the more, the better! Once the algorithm has learned a bit about what a correct page of handwritten text looks like, a model is then created which contains all the rules and parameters and calculations ready for the particular task I have set it of predicting what handwritten text is on pages of this particular collection that it’s never seen before!\n\n\n\n\nOk, quick quiz time!\nRemembering that Machine Learning is about prediction, which of the following do you think would require ML?\n\nCounting the number of people in a museum using information from entry and exit barriers.\nA search system that looks for images similar to a user submitted sketch.\nA system that recommends library books based on what other users have ordered.\nA queueing system that spreads people evenly between 5 ticket booths\nA program which extracts names from documents by finding all capitalised words and checking them against a list of known names\nA system which turns digitised handwritten documents into searchable text\nA robot that cleans the vases in a museum without bumping into them or breaking them\n\nIf you answered 2, 3, 6 & 7 you are correct! The others could all be most easily executed with a straightforward algorithm, programmed using a simple set of easily defined rules, rather than requiring prediction.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "ml-ai.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "AI & ML in Libraries Literacies",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nAs you now know, machine learning algorithms and models underpin all the other subfields of AI and there are a LOT of subfields of AI depending on who you talk to! In this guide though we’ll focus our attention on just these two particular AI research areas to give you a general sense of how machine learning is practically applied in the library context today:\n\nNatural Language Processing (NLP): which is concerned with making AI systems more capable of natural and effective interaction with humans (text and speech)\nComputer Vision (CV): which is concerned with enabling machines to interpret and make decisions based on visual data from the world.\n\nLet’s have a look at these through some use cases:\n\nNatural Language Processing (NLP)\nNLP involves the development of a wide range of algorithms, models, and systems for analysing, understanding and extracting meaningful information from textual and speech data representing human language. We can use NLP for things like:\n\nSubject indexing to enhance library catalogue search\nNamed Entity Recognition (NER) is a text analysis process within NLP that helps turn unstructured text into structured text. A sentence or a chunk of text is parsed through to find entities that can be put under categories like names, organisations, locations, quantities, monetary values, percentages, etc.\n\nIn the library world it can be used as part of a process to understand what subjects (people, places, concepts) are contained within a digitised text and help us enhance our catalogue records for items or search functionality. There is a very nicely outlined use case here of how the United States Holocaust Memorial Museum used NER to automatically extract person names and location from Oral History Transcript to improve indexing and search in their catalogue.\n\n\nAutomatic Language Detection & Genre Classification\nThe British Library has used different machine learning techniques and experiments derived from the field of NLP to assign language codes and genre classification to catalogue records, in order to enhance resources described. In the first phase of the Automated Language Identification of Bibliographic Resources project, language codes were assigned to 1.15 million records with 99.7% confidence. The automated language identification tools developed will be used to contribute to future enhancement of over 4 million legacy records. The genre classification case study includes a nice description of machine learning as well as references to other use cases for metadata clean up.\n\n\nText & Conversation Generation\nLanguage models are a type of machine learning model designed to predict the likelihood of a sequence of text, which means that they can be set up to predict the most likely way to continue a conversation. A large language model (LLM), such as the models behind ChatGPT, are highly complex neural networks that have been exposed to an enormous amount of text from books, articles, websites, and more. They perform natural language processing tasks such as generating and classifying text, answering questions, and translating text and are the backbone of NLP today. On the other hand there are small language models (SML) too which, well, you guessed it, are much smaller, as in, they don’t need quite as much data and time to be trained. Whether or not to use either depends on your use case and motivations!\nThere are and have been for many years, large language models out there actually but ChatGPT has currently caught the popular imagination because of its publicly available interface and remarkable performance, so it’s worth spending a little time here unpacking just what ChatGPT is and its potential impact on library work.\nThe large language models behind ChatGPT have learned something about patterns in grammar and word meaning, including the way that meaning arises contextually across multiple sentences and multiple turns in a conversation. When you ask ChatGPT a question, you are presenting the model with new information it tries to make a prediction on, in this case, it tries to generate a response that matches the pattern of conversation.\nYou ask questions or give prompts to the model, and it provides responses in natural language, or rather, estimates what should come next in a conversation. When ChatGPT gives a response, it isn’t actually looking up information and then composing that information into a response; it’s just making an estimation of a response based on patterns it has seen. So, when you ask it factual questions, especially ones with common answers or phrases, it might give you an answer that sounds right but remember this is because it’s mimicking what it has seen in its training data.\nLibrarians are still investigating use cases for this new Generative AI applications, but for now at least, ChatGPT is certainly useful as a personal writing assistant or tool to help give you ideas for\n…creating a title for a new exhibition\n…creating exhibition labels\n…outlining a basic structure for an information literacy workshop\n…creating a blog post on a topic for which you are very familiar\n…helping you reword something for different audiences\n…writing a funding proposal!\nIt’s also good to get in the habit of trying out and being aware of how these particular models work as more and more library users will be using this technology too, and may not know quite have a clear understanding of what’s behind the responses generated by them. We’ve seen librarians having to answer queries about citations that have been made up by ChatGPT, article references which sound very much like they exist, but have just been hallucinated by the model!\n\n\n\nComputer Vision (CV) use cases\nWe can use computer vision to train models to automatically analyse and understand useful information from images and videos.\nIn the library world we can use this to label millions of images, or, as we see below, a model can be trained to classify this image as a newspaper based on objects identified in the layout (for example, a nameplate for the newspaper, a headline, photographs, and illustrations and so on). The model learns how to identify that this is the NYT based on learning from other newspaper images it’s seen (for example, if given NY Tribune, NY Times, and NY Post images, it can distinguish between the various titles).\n\n\n\nPutting it altogether: ML + CV + NLP\nOne of the state of the art applications of machine learning seen in cultural heritage at the moment is Handwritten Text Recognition (HTR) which we had a look at earlier briefly in our simple example of training a model. The idea with HTR is to convert digitised handwritten documents into searchable and machine readable text. To achieve this HTR actually uses a combination of Computer Vision (CV) and Natural Language Processing (NLP).\nSince handwriting can be tricky and ambiguous you might have a Computer Vision model try to identify possible letters from the shapes, and another to work out what the most likely word is from those shapes. But let’s imagine that there’s a smudge on the page, and the letters and maybe even whole words are completely illegible. In that case you might turn to your NLP language models which look at sentence level predictions, taking into account words in the whole line of text and use that context to work out what words are most likely missing in those smudged spots!\nSometimes a model trained for a particular task (in the case of this HTR example, identifying a particular handwriting style), it can be applied to similar handwriting styles with very good results. Transkribus has Public AI Models (transkribus.org) that are shared and can be reused by anyone.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "ml-ai.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "AI & ML in Libraries Literacies",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe following quick little hands-on activities below were developed by the Digital Research Team for British Library staff as part of the Digital Scholarship Training Programme.\n\nActivity: Explore Natural Language Processing\nCopy and paste a paragraph of text from somewhere around the web, or from your own collections, and see how each of these cloud services handle it:\n\nCloud Natural Language\nIBM Watson Natural Language Understanding Text Analysis\ndisplaCy\nVoyant Tools (voyant-tools.org) Voyant Tools is an open-source, web-based application for performing text analysis. It supports scholarly reading and interpretation of texts or corpus, particularly by scholars in the digital humanities, but also by students and the general public. It can be used to analyse online texts or ones uploaded by users.\nAnnif - tool for automated subject indexing There are many video tutorials here and the ability to demo the tool\n\n\n\nActivity: Explore ChatGPT\nLogin to use the freely available ChatGPT (openai.com) interface.\nTo get a useful response from ChatGPT, “prompting” is key. If you only ask a simple question, you may not be happy with the results and decide to dismiss the technology too quickly, but today’s purpose is to have a deeper play in order to develop our critical thinking and information evaluation skills, allowing us to make informed decisions about utilising tools like ChatGPT in our endeavours. Basics of Prompting | Prompt Engineering Guide (promptingguide.ai) gives a nice quick walk-through of how to start writing good prompts or you can take a free course !\nHave a play trying to get ChatGPT to generate responses to some of the questions here (or come up with your own questions!) Critically evaluate the responses you receive from ChatGPT, what are its strengths and weaknesses, ethical considerations and challenges of using AI tools such as this.\n\nIs the information/response credible?\nAre there any biases in the responses?\nDoes the information align with what you know from other sources?\n\n\n\nActivity: Explore Computer Vision\nFind an image from somewhere on the web, or from your own collection, and see how each of these cloud services handles it! Try with some images of basic objects to see results (cars, fruit, bicycles…) and images with text within them.\n\nGoogle Cloud Vision API – https://cloud.google.com/vision/docs/drag-and-drop\nTranskribus https://www.transkribus.org/ (Try for free, but does require a free account to login)\nVisual Geometry Group - University of Oxford",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#recommended-readingviewing",
    "href": "ml-ai.html#recommended-readingviewing",
    "title": "AI & ML in Libraries Literacies",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMuch of this topic guide is based on both a Library Carpentry Intro to AI for GLAM lesson I have developed with colleagues and a 2024 talk I gave as part of a Research Libraries UK Digital Shift Forum so if you’d like to view/listen to the content covered in this topic guide as a lecture:\n\n\n\nWatch the video\n\n\nThere are of course untold numbers of lists out there with resources for learning more about AI & Machine Learning but I think this particular guide is exceptionally useful in its coverage and topics selected, particularly as they are quite specifically for Librarians: Add’tl Reading for Librarians & Faculty - Using AI Tools in Your Research - Research Guides at Northwestern University",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#taking-the-next-step",
    "href": "ml-ai.html#taking-the-next-step",
    "title": "AI & ML in Libraries Literacies",
    "section": "Taking the next step",
    "text": "Taking the next step\nThe AI4Lam group is an excellent, engaged and welcoming international organisation dedicated to all things AI in Libraries, Archives and Museums. It’s free for anyone to join and is a great first step for anyone interested in learning more about this topic!",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "collectionsasdata.html",
    "href": "collectionsasdata.html",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "Introduction: What is the concept of Collections as Data and how can my institution get started?\nGLAM (Galleries, Libraries, Archives and Museums) institutions have been making their content available to the public and researchers in a variety of formats for different purposes for decades. From maps, images, text, historical newspapers or postcards, digitising collections and displaying our items online are at the core of making this information accessible for research and enjoyment. But recent advances in and expansion of the use of AI and Machine Learning in library work and library research have transformed the way in which we and our users expect to access, view, use and search our collections. As the demand for computationally accessible cultural heritage collections continues to grow, and we’re being asked to provide our collections in the form of datasets for instance, how do we go about this practically?\nIn this new context, where GLAM institutions now find themselves playing a leading role as data providers and data curators, the Collections as Data concept emerged as a new approach, guidance and framework to support responsible development and computational use of digital cultural heritage collections. By computational use we mean the application of computational techniques such as natural language process, computer vision, and more to the analysis, exploration and reuse at scale of digitised cultural heritage content. For more practical use cases see our AI and Machine Learning in Libraries topic guide.\nThree particularly useful outputs from the Collections as Data community of practice are:\n“Want to support collections as data at your institution, but not sure how to begin? Drawing on what we learned from engaging with practitioners and researchers throughout the Always Already Computational project, the project team compiled a list of 50 Things you can do to get started. 50 Things is intended to open eyes, stimulate conversation, encourage stepping back, generate ideas, and surface new possibilities. If any of that gets traction, then perhaps you can make the case for investing in collections as data at your institution in a meaningful, if not systematic, way”\n“The Vancouver Statement suggests a set of principles for thinking through questions that collections-as-data work produces, as part of an expanding global, interprofessional, and interdisciplinary effort to empower memory, knowledge, and data stewards (e.g., practitioners and scholars) who aim to support responsible development and computational use of collections as data. This stewardship role only grows in importance as artificial intelligence applications, trained on vast amounts of data, including collections as data, impact our lives ever more pervasively.”\nDeveloped as a result of a community-led effort, the checklist covers different aspects such as providing a suggested citation, including documentation about the datasets (e.g., README files or tutorials) or sharing examples of use (e.g., prototypes or Jupyter Notebooks).",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#introduction-what-is-the-concept-of-collections-as-data-and-how-can-my-institution-get-started",
    "href": "collectionsasdata.html#introduction-what-is-the-concept-of-collections-as-data-and-how-can-my-institution-get-started",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "Fifty things (2018)\n\n\n\nVancouver Statement on Collections as data (2023)\n\n\n\nA Checklist to Publish Collections as Data in GLAM Institutions (2023)",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Collections as Data: Getting Started",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nMany institutions have started to adopt the collections as data principles to varying degrees, often combining publication of digital collections suitable for computational use, with a lab offering technical support to use them. You might notice that the examples below are all slightly different implementations of the principles though as each institution will necessarily need to decide for themselves, based on their individual goals and constraints (such as lack of resources, staff or IT skills) how to practically adopt the Collections as Data principles.\n\nData Foundry at the National Library of Scotland provides datasets in the form of downloadable files. Each of them includes a website with transparent information about the creation and curation of the content. It also provides examples of use and prototypes in the form of Jupyter Notebooks.\nNational Library of Luxembourg provides a digital collection based on Historical newspapers. It follows an innovative approach by providing different datasets in terms of the size and the content according to the purpose of the reuse (e.g., getting started or Big Data).\nBritish Library Labs provides a selection of datasets made available under open licences to experiment.\nDATA-KBR-BE is a project that aims at facilitating data-level access to KBR’s digitised and born-digital collections for digital humanities research.\nBiblioteca Virtual Miguel de Cervantes (BVMC Labs) is a digital library that published its bibliographic catalogue in the form of Linked Open Data. It also provides examples of use by means of of Jupyter Notebooks.\n\nMore advanced approaches are focused on the concept of data spaces, which are new cloud environments in which data can be shared for research use, while also allowing data suppliers to retain rights and control over the data. This can be useful for contexts in which the access of the data is restricted (e.g., geographic region, licensed content). A workflow to publish Collections as Data: the case of Cultural Heritage data spaces ) gives more information on this topic in the context of the European common data space for cultural heritage.\nIt is important to note that institutions need to carefully consider how and for what purpose they want to publish their digital content. For instance, the National Library of the Netherlands has recently restricted access to collections for training commercial AI. See our topic guide on Copyright and Licensing for more on that.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Collections as Data: Getting Started",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nOne simple activity you could do at your institution is to gather a group of like-minded colleagues together and have a read through Fifty things, Vancouver Statement on Collections as data and/or A Checklist to Publish Collections as Data in GLAM Institutions and have a look at some of the case studies above, and as a group consider the questions:\n\nAre there some activities suggested that are already underway?\nCan you identify one or two simple things that your institution could do now in order to start using these principles?\nWhat might you like “collections as data” to look like at your institution?\n\nIf you’d like to get a sense of how people are reusing collections published as data, the GLAM Workbench and the new computational access section of the International GLAM Labs Community website both have tutorials that can walk you through using cultural heritage datasets in a variety of ways.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#recommended-readingviewing",
    "href": "collectionsasdata.html#recommended-readingviewing",
    "title": "Collections as Data: Getting Started",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in reading more about the collections as data concept we highly recommend the Zotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets which is an open bibliography collaboratively maintained by the community of practice of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#taking-the-next-step",
    "href": "collectionsasdata.html#taking-the-next-step",
    "title": "Collections as Data: Getting Started",
    "section": "Taking the next step",
    "text": "Taking the next step\nJoining the International GLAM Labs Community is a great way to get started on the path of opening your collections up as data as well as the Collections as Data - Google Group as both communities are very active and have a wide range of experience and expertise to share on this topic.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "lod.html",
    "href": "lod.html",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "Introduction: What does publishing linked open data enable?\nThe Semantic Web was first introduced in the 2000s by Tim Berners Lee as an extension of the current Web. Instead of providing information in the form of documents and unstructured text like in traditional webpages, the Semantic Web facilitates the publication of machine-readable data on the web through standards such as Resource Description Framework (RDF) and Web Ontology Language (OWL).\nLinked Open Data (LOD) is a method of publishing structured data about things using the RDF to enable interlinking and semantic queries across datasets. The data is organised in “triples”, each consisting of a subject (e.g., Named Person), predicate (IsAuthorOf), and object (Book Title), identified by Uniform Resource Identifiers (URIs) to ensure global uniqueness and interoperability. It allows metadata to be connected and enriched, so that different representations of the same content can be found, and links made between related resources.\nHave a quick look at this video from Europeana explaining the high-level basic principle of LOD before we dive a bit deeper into how it practically works:\nLinked Open Data | Europeana PRO",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#introduction-what-does-publishing-linked-open-data-enable",
    "href": "lod.html#introduction-what-does-publishing-linked-open-data-enable",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "How it works\nRDF triples are the fundamental building blocks of Linked Open Data. Triples follow the RDF standard and consist of three components:\n\nSubject: This is the entity or resource being described. It is usually represented by a URI that uniquely identifies the resource.\nPredicate: This represents the relationship or property of the subject. It is also identified by a URI and specifies the type of relationship between the subject and the object.\nObject: This is the value or resource that is related to the subject. The object can be another URI (representing another resource) or a literal value (such as a string or number), amongst others.\n\nAn example of a triple stating “Miguel de Cervantes is author of El Quijote.” would look like this:\n\nSubject: &lt;https://www.wikidata.org/wiki/Q5682&gt; (Wikidata URI reference to Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core URI term for creator/author)\nObject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata URI reference to the book El Quijote)\n\nIn a 2020 survey of LIBER members, the LIBER Linked Open Data Working Group identified the following as the most frequently used datasets by libraries to enrich their catalogues but there are many more that can be used depending on your needs Linked Data Survey (oclc.org) and some examples of advanced data models are Bibliographic Framework (BIBFRAME) and Library Reference Model (LRM). In addition, the lod-cloud provides more than one thousand LOD repositories classified by categories and based on different domains such as geography and government.\n\nSo let’s go back to our triple that describes the relationship between the resource “Miguel de Cervantes” and the book of “El Quijote”. When different triples share the same URI for a subject, predicate, or object, they create a connection. For example:\nTriple 1: Miguel de Cervantes is author of El Quijote\n\nSubject: &lt;https://www.wikidata.org/wiki/Q5682&gt; (Wikidata identifier for the author Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core term for creator/author)\nObject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata identifier for the work El Quijote)\n\nTriple 2: El Quijote is a work of Spanish Literature\n\nSubject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata identifier for the work El Quijote)\nPredicate: &lt;http://purl.org/dc/terms/subject&gt; (Dublin Core term for subject)\nObject: http://dbpedia.org/resource/Spanish_literature\\ (DBpedia identifier for Spanish literature)\n\nHere, the object of the first triple (&lt;https://www.wikidata.org/wiki/Q480&gt;) is the subject of the second triple, linking information about the book to information about its subject matter. So an example catalogue record combining many triples then might look like:\n&lt;http://example.org/catalogue/El_Quijote&gt; &gt;rdf:type schema:Book; schema:name “El_Quijote”; schema:author &lt;https://www.wikidata.org/wiki/Q5682&gt;; schema:genre &lt;http://dbpedia.org/resource/Novel&gt;; schema:inLanguage &lt;http://id.loc.gov/vocabulary/iso639-1/es&gt;; schema:datePublished “1605”; schema:about &lt;http://dbpedia.org/resource/Spanish_literature&gt;; schema:about &lt;http://dbpedia.org/resource/Spanish_Golden_Age&gt;; schema:sameAs &lt;http://dbpedia.org/resource/Don_Quixote&gt;.\n&lt;https://www.wikidata.org/wiki/Q5682&gt; &gt;rdf:type schema:Person; schema:name “Miguel de Cervantes”; schema:birthPlace &lt;http://dbpedia.org/resource/Alcala_de_Henares&gt;; schema:birthDate “1547-09-29”.\n&lt;http://dbpedia.org/resource/Alcala_de_Henares**&gt; &gt;rdf:type schema:Place; schema:name “Alcalá de Henares”; geo:country &lt;http://sws.geonames.org/2510769/&gt;.\n&lt;http://sws.geonames.org/2510769/&gt; &gt;rdf:type schema:Country; schema:name “Spain” .",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Linked Open Data in Library Use Today",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nGLAM institutions and in particular, libraries, have played a leading role in the publication of their data, primarily collections metadata, as LOD and using them including:\n\nBibliothèque nationale de France\nBiblioteca Virtual Miguel de Cervantes\nBritish Library\nEuropeana\nLibrary of Congress\nNational Library of Scotland\nNational Library of Spain\n\nAdditional examples from other related domains such as museums and Digital Humanities initiatives are the Rijksmuseum and Smithsonian American Art Museum, and Linked Open Data Infrastructure for Digital Humanities in Finland (LODI4DH).\nThe benefits of the publishing and use of the Semantic Web and LOD for:\n\nSemantic Enrichment: LOD helps libraries improve searchability and enables more precise queries by enriching existing catalogue records. Libraries have started to enrich their catalogues with external LOD repositories in order to provide additional contextual information that may be missing from your own catalogue (e.g., author nationalities (VIAF), geographic coordinates (GeoNames) relating to birth places of authors, or related subjects (Library of Congress Subject Headings). As in the example above a catalogue record for the book “El Quijote,” could be enriched with metadata about the author, language, publication date, related literary movements, and geographical information, all connected through LOD triples.\nInterconnectedness: LOD allows libraries to link their data with other rich datasets, creating a web of interconnected information. This enables users to discover related resources beyond their own library’s holdings. For example: a library could link their catalogue data with other LOD repositories, to enhance search results. Searching for “El Quijote” in the catalogue could return results not only from their own collection but also from other institutions that use LOD.\nIncreased Visibility: By publishing data as LOD, institutions can increase their visibility on the web as researchers, developers, and other institutions can easily find and reuse library data. For example: Adding information about a rare copy of El Quijote in your collection to Wikidata would aid its discovery through Wikipedia articles (Libraries and Wikidata: Using linked data to expand access to library collections worldwide – Wiki Education).\nInnovation: LOD encourages creative applications and tools. Developers can build new services, visualisations, and applications using linked library data. For example: LOD allows the creation of new types of visualisations, such as timelines, maps and graph charts that can be useful to gain insight, in some cases without the need to install additional software thanks to the use of APIs. Some examples include:\n\na tutorial in Spanish to create map visualisations based on Wikidata and using several data repositories (e.g., members of the International GLAM Labs Community) as content\nthe exploration of machine-readable visual configurations to browse LOD repositories provided by Cultural Heritage institutions, including libraries, in the form of Jupyter Notebooks\na map representing the geographic locations mentioned in the metadata provided by a corpus of historical documents and paintings.\n\n\nThough there are many benefits, SPARQL is the means by with Linked Open Data is queried and accessed and it’s worth being aware that the use of API’s based on SPARQL can be complex for less technical users since they need to understand how the data is modelled as well as be able to type a query. In addition, data quality has become crucial and several initiatives are focused on the assessment of the data quality provided by the catalogues.\n\nCase Study: Manuscripts on Wikidata: the state of the art? | by Martin L Poulter | Medium\nThis example shows how to use Wikidata, a community-driven approach based on the Semantic Web and LOD that enables volunteers to edit the metadata, to describe manuscripts. It shows the expressivity of the vocabulary provided by Wikidata and the benefits of using Wikidata as a repository in terms of visibility and reuse.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Linked Open Data in Library Use Today",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nAs part of my National Research Librarian’s fellowship at National Library of Scotland exploring the adoption of Semantic Web technologies to transform, enrich and assess the Data Foundry’s digital collections, I created a collection of Jupyter Notebooks that enables users to:\n\nunderstand the benefits of the adoption of the Semantic Web;\ncreate an RDF repository from a traditional dataset;\nenrich a dataset with external repositories such as Wikidata;\nreproduce the analysis and visualisations based on the datasets created.\n\nI can also highly recommend starting with this Introduction to the Principles of Linked Open Data | Programming Historian tutorial which gives a great walk through of creating linked open data and includes an activity for using SPARQL to query LOD.\nThe course about Linked Open Data in cultural heritage collections, developed at Leiden University also includes a tutorial about a number of tools that can be used to create and to publish LOD. More specifically, it contains discussions of the LDwizard and CLARIAH Data Legend tool ‘COW’.\nTo be able to retrieve and analyse Linked Open Data, you need to know how to build SPARQL queries. The following course can be helpful:\n\nIntroduction to SPARQL\n\nExamples of SPARQL queries used to collect and analyse data from heritage institutions can be found in the notebooks below:\n\nThe Europeana SPARQL endpoint\nWikidata\nShort Title Catalogue of the Netherlands\nThe Dutch Institute for Art History",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#recommended-readingviewing",
    "href": "lod.html#recommended-readingviewing",
    "title": "Linked Open Data in Library Use Today",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in learning more about LOD in terms of how to transform traditional bibliographic information into the Semantic Web, check out my research work performed as part of a fellowship at the National Library of Scotland in order to publish digital collections as LOD.\nI can also recommend the excellent Best Practices for Library Linked Open Data (LOD) guide published by the LIBER Linked Open Data (LOD) Working Group in 2021 which outlines in detail six steps for publishing library linked data.\n\nSome examples of research articles to read providing additional details and information include:\n\nTowards a semantic approach in GLAM Labs: The case of the Data Foundry at the National Library of Scotland\nLIBER’s Linked Open Data Working Group Publishes ‘Best Practices for Library Linked Open Data (LOD) Publication’ - LIBER Europe\nAn automatic data quality approach to assess semantic data from cultural heritage institutions\nA Shape Expression approach for assessing the quality of Linked Open Data in libraries\nAn Ontological Approach for Unlocking the Colonial Archive. (Example of transformation into RDF of a collection of maps using Open Refine.)\nEvaluating the quality of linked open data in digital libraries - Gustavo Candela, Pilar Escobar, Rafael C Carrasco, Manuel Marco-Such, 2022 (sagepub.com)\n\nYou can also find innovative ideas in the research articles published at the Semantic Web Journal.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#taking-the-next-step",
    "href": "lod.html#taking-the-next-step",
    "title": "Linked Open Data in Library Use Today",
    "section": "Taking the next step",
    "text": "Taking the next step\nThe LD4 Community is a community of practice for linked data in libraries.\nLinked Art is a community working together to create a shared model based on LOD to describe cultural heritage with a particular focus on art.\nCode4Lib is a community effort including a mailing list and a journal providing open articles based on the library domain and including LOD.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "iiif.html",
    "href": "iiif.html",
    "title": "IIIF",
    "section": "",
    "text": "Introduction: What is IIIF?\nIIIF (pronounced “triple-eye-eff”) stands for the International Image Interoperability Framework. Quite a tongue twister that one, but it broadly represents two things:",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#introduction-what-is-iiif",
    "href": "iiif.html#introduction-what-is-iiif",
    "title": "IIIF",
    "section": "",
    "text": "a set of open standards for delivering high-quality, attributed digital objects online at scale.\nthe open, international community of software developers, libraries, researchers, educators, museums, universities, creative agencies, and more working together to develop and implement the IIIF APIs to make the above open standards happen.\n\n\nOpen Standard\nEver try to look at a large high-resolution digitised manuscript online only for it to take ages to load, and when it finally does you have no way to actually move around the image easily nor see any of the metadata or annotations related to it?\nOr maybe spent months on end negotiating the terms and methods around sending copies of a variety of differently sized individual images to another institution for them to host as part of collaborative project?\nIIIF brings a whole new efficiency to the way in which we in the cultural heritage sector go about sharing and making our digitised collections available online, while greatly expanding the functionality around the way users interact with them. It’s an open standard, collaboratively developed and maintained by a host of cultural heritage institutions around the world that defines a consistent method for the delivery of images and audio/visual files from servers to different environments on the Web where they can then be viewed and interacted with in many ways.\nIIIF basically specifies a way for browsers to display an image or audio/visual files in a way that enables much richer functionality on the Web:\n\nMakes it easier to display large images on the web in a way that is scalable (enabling deep zoom)\nAllows easy comparison between two objects, connecting and uniting materials across institutional boundaries\nDisplays structure and metadata and annotations with the digital collection item. (For a digitised manuscript for instance this might be page order and searchable text, for audio/visual materials, that means being able to deliver complex structures (such as several reels of film that make up a single movie) along with things like captions, transcriptions/translations, annotations, and more.)\n\n\n“At its simplest, IIIF uses APIs to load images quickly and zoom smoothly without additional loading time. But IIIF also allows you to do much more, including pulling IIIF-enabled images from different sites into viewers for comparison without downloading them (at full resolution), and enabling saving links to details of images or portions of A/V files for future reference. IIIF also allows you to use many open-source tools that help you to compare, annotate, transcribe, collaborate, and more. You can even gather multiple IIIF images together from across multiple archival collections and/or institutions to create projects or exhibits without advanced technical skills”. -From IIIF for Archives\n\nMaking your collections IIIF enabled makes it easier to share your content online in a consistent way, enabling portability across different IIIF enabled viewers. This means that rather than endlessly creating copies of your images all over the world for different projects, they can stay on your same IIIF Image server, but be accessed and displayed by viewers hosted at institutions elsewhere.\n\n\nOpen Community\nThe IIIF community is made up of over 100 major cultural heritage organisations worldwide who have formally adopted it, including many of our very own LIBER members. It was started in 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. It’s a really nice example of an open, grassroots but global community effort, backed by a consortium of leading cultural institutions, who have been working together to develop and implement this new capability for decades and solve their shared problems with delivering, managing, sharing, and working with their resources.\n\n\nWhat would I need to do to make my collection IIIF enabled?!\nIf you want the deep nitty gritty technical stuff around all the APIs and how they fit together there is quite a bit of implementation documentation on their website that goes into all this. But essentially, the basic set up behind making your own digitised collections “IIIF enabled”, as they say, looks a little something like this:\n\nSet-up a IIIF image server (you can choose one developed by the community, or there are IIIF-compatible image servers available from vendors or other web hosts), move your content there and implement the IIIF Image API to make those images and audio/video materials available from there.\nImplement the IIIF Presentation API which creates the all important IIIF Manifest files (also many open source or vendor products can help handle this bit too) for each of your objects. This Manifest file is really the prime unit in IIIF, it essentially combines and packages in json code, information about your images and structural data from your metadata source. It lists all the information that makes up your new IIIF enabled object, from how to display it to what information IIIF viewers should (and should not) display (such as structure or the order of pages, or even as minute as where an illustration is located within an image if you like). If you want to see an example of what one looks like this is a IIIF Manifest from the Bodleian Libraries at University of Oxford relating to this collection item. Each manifest has its own URL and that’s the bit you’ll use to do cool things with the object in different IIIF viewers, such as allowing a manuscript to be easily dragged and dropped into Mirador for instance for comparison with other IIIF enabled manuscripts.\nChoose one of the many IIIF enabled viewers for displaying your images and add it to your own collection site. Again, looking at that same collection item record above, note in the upper left hand-corner (see image below) there is an option to view in Mirador or Universal Viewer which are two different styles of IIIF viewer that afford different functionalities.\nConsider making your IIIF Manifests available publicly for download so users can work with them in all the interesting ways you’ve now enabled!",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "IIIF",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nIIIF was initiated by a bunch of libraries and cultural heritage collections holders and it shows! It was proposed in late 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. The intention of the consortium has always been to combine resource and effort in building viewers that reflected the way we wanted our digital collections to be displayed online, rather than everyone still spending time and resource making our own custom viewers and building our own content silos. It’s brought a huge amount of efficiency too in the way that we share images with each other and researchers. It’s changed the way collaborative projects are undertaken where endless metadata mapping exercises, contract negotiations around re-use and hosting, and the copying and shipping of digitised materials on external hard drives were the norm.\nThere are quite a number of use cases and case studies available on the IIIF Demos page but let’s have a quick look at three real life (and canonical) examples of IIIF in action.\n\nDeep Zoom and Annotations\nThe example here is of the Ōmi Kuni-ezu 近江國絵圖 Japanese Tax map created in 1837. It’s meant to be read in the round by someone standing in the middle–you can see the scale when this zooms out– the map is eleven by seventeen FEET, and the person standing next to it, Wayne who works in the library at Stanford, is six feet four inches tall. This image is a composite of 158 individual images with a file size of 1.27Gb. IIIF allows just enough of an image to be delivered to a viewer–going from a whole image to just the part that they are zooming in on. Without IIIF, an end user might have to download an extremely large file, but using IIIF provides a smooth and easy viewing experience.\n\nHave a play and view this image in their Universal viewer here\n\n\nVirtual Reconstruction\nThe virtual reconstruction of this damaged manuscript from Châteauroux in France (Grandes Chroniques de France, ca. 1460) is probably one of the most well-known and best examples of the power of IIIF to support this use case (and my own personal favourite!). At some point in the manuscripts history, fourteen of its illuminations were cut out. These illuminations eventually ended up at the Bibliothèque nationale de France in the 19th century and were digitised individually. In the demo you see the reuniting of the miniatures with the full manuscript as IIIF allows a virtual repositioning of the cut out decorations with the text, virtually reconstructing the manuscript online using the Mirador Viewer so it reflects its original state.\n\nI highly recommend having a play around with the Mirador Viewer: Châteauroux demo.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "IIIF",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe following tutorials are two of my favourite recommendations for colleagues interested in having a play with IIIF manifests yourself and in the process gaining a practical understanding of how they work, and the benefits!\n\nIIIF Online Workshops The community itself has created a number of excellent and free self-paced tutorials and though they host live online workshops for a fee, these are recorded and available and useful for newcomers for free afterwards. In fact staff at British Library have walked through these self-guided resources online quite often with great success. There is also the opportunity to hire a IIIF Trainer to come to deliver live bespoke training directly to your institution (for a fee), which we have also partaken in!\nWorking with IIIF images in education, communication and research This is a self-guided workshop available online in Dutch and English and has some excellent exercises to get you familiar with finding IIIF manifests in catalogues and importing them into different viewers. I highly recommend making some time (they recommend 120 minutes) to read through this and try out some of the exercises.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#recommended-readingviewing",
    "href": "iiif.html#recommended-readingviewing",
    "title": "IIIF",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere is a useful collection of articles and data related to IIIF being compiled by the community at https://zenodo.org/communities/iiif/records?q=&l=list&p=1&s=10&sort=newest\nThe IIIF organisation has also created a number of useful resources alongside their training materials such as How It Works, a plain-language guide to how the IIIF API’s work and a glossary of “Key concepts you’ll encounter when working with IIIF”",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#taking-the-next-step",
    "href": "iiif.html#taking-the-next-step",
    "title": "IIIF",
    "section": "Taking the next step",
    "text": "Taking the next step\nLearning about IIIF can be overwhelming at first, especially if you’re not a programmer, but the IIIF Community is a very supportive and engaged one and has created a number of ways to get involved and find support and help.\nI recommend checking out their community page IIIF Community to find details of their next open community calls, or to join their Slack Channel where you can post questions and join the discussion with other users.\nThere is also a massive list of resources, Awesome IIIF, compiled and maintained by the IIIF Community if you are looking to take your knowledge a bit further and dig deeper into some of the exciting implementations of IIIF.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "What is an API?",
    "section": "",
    "text": "Introduction\nAny time you write code you are using an API! When you write print(“Hello world”) in Python, the Python Standard Library API is the contract that guarantees the words Hello world will appear on the screen. When you do the same in R, it’s the API of R’s base package that you are using. Other languages’ APIs use other words for the same function (printf, echo, format are some common ones) and may have subtly different interpretations.\nIn many ways an API is like a restaurant menu: the menu sets out the acceptable ways of interacting with the kitchen by specifying what dishes you can order with a brief description of what to expect from the dish, and how much you will have to pay. Items on a menu don’t include a detailed description of their implementation (a recipe), only enough information for the diner to make an informed choice about what they want to eat. If you’re confident you know what you’re doing you can order off-menu, but the results aren’t guaranteed to be what you expect, because there is no documentation of those expectations for the kitchen to refer to.\nAPI is an acronym for Application Programming Interface. Where a GUI (Graphical User Interface) provides a consistent way for humans to interact with an application, an API provides a consistent way for code to interact with other code. APIs set clear expectations that are necessary for\nGenerally “API” can be applied to any consistent interface provided by one computer program for other programs to use, but the most common you are likely to encounter in Digital Scholarship work are:\nBoth of these examples share some important elements. They specify:\nLet’s look at those two types in more detail through this lens.",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#introduction",
    "href": "api.html#introduction",
    "title": "What is an API?",
    "section": "",
    "text": "A set of functions/variables available through a code library, and their expected behaviour\n\nE.g. the Python Standard Library’s API includes a module called re which provides a number of functions, classes and constants that allow you to work with regular expressions\n\nA set of HTTP URLs provided by a system/service, and the expected behaviour when those URLs are requested from the server by another program\n\nE.g. the DataCite API defines a URL https://api.datacite.org/dois which returns a list of all published DataCite DOIs to the requester, formatted as JSON, and when followed by the parameter ?prefix=10.23636 will filter the returned list to only include DOIs with that prefix\n\n\n\n\nWhat the programmer must do\nWhat response the programmer can expect and in what format\nWhat side-effects the programmer should expect\n\n\n\nExample 1: a Python library API\nThe re module mentioned above includes a function called search with the following documentation:\n\nre.search(pattern, string, flags=0)\nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding Match. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.\n\n\n\n\n\n\n\nTodo\n\n\n\n\nBreak down this definition into how it describes different parts of how the programmer can use this function\nMaybe move this section to the hands-on activity?\n\n\n\n\n\nExample 2: a HTTP API\n\n\n\n\n\n\nTodo\n\n\n\n\nWorked example of a HTTP call (e.g. the DataCite endpoint mentioned above)\nA diagram will be very helpful!\nShould this include example Python code or is that too much detail?",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#glossary",
    "href": "api.html#glossary",
    "title": "What is an API?",
    "section": "Glossary",
    "text": "Glossary\n\nArgument\nCall\nClient\nHTTP\nLibrary\nParameter\nRequest\nReturn\nServer",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "What is an API?",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\n\nMany (most?) systems that we use every day have APIs. Knowing what these look like and how to use them in general is an important first step in being able to write software that accesses those systems to do much more than you could do with your own code alone. This becomes even more powerful when you can write “glue” code that brings together parts of two or more APIs",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "What is an API?",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\n\n\nSomething involving Postman?\nExploring published API documentation? E.g. DataCite API docs allow you to run test queries from the web page",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#recommended-readingviewing",
    "href": "api.html#recommended-readingviewing",
    "title": "What is an API?",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\n\nWikipedia’s “API” entry is a good jumping off point, including some history\n“HTTP: Learn your browser’s language!”: excellent zine by software engineer Julia Evans (costs 12 USD)",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#taking-the-next-step",
    "href": "api.html#taking-the-next-step",
    "title": "What is an API?",
    "section": "Taking the next step",
    "text": "Taking the next step",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualisation",
    "section": "",
    "text": "Introduction\nWhy do we visualise data? For data to tell us something we need to look for patterns, and we are much better at finding these patterns in colours and shapes than in a table of raw data. Visualisations are key to developing the story we want to tell with our data.\nWhen do we visualise data? There are two main moments when working with data that we need to visualise it. First, in the exploratory phase, when we are trying to understand the data, to get insights that lead to fruitful lines of enquiry. These visualisations are rough and ready, meant for us or at most a select few around us to generate debate about what information the data might contain.\nSecond, in the explanatory phase, when we have understood the data and move on to insightful analyses that generate new understanding. These visualisations communicate what we have learned from the data to others. They need to be clear, because we are trying to explain conclusions we have made from intimate knowledge of the data to people who have not worked with it, and are trusting us to explain faithfully what we have learned.\nIn libraries this covers visualisations for users of our services designed to improve content discovery and so they can understand our data, as well as research carried out by librarians and digital humanists that generates new knowledge. Particularly for our users, whose needs should be at the heart of the services we supply, visual routes we supply into our data need to be as clear as anything we’re presenting as research.\nHow do we visualise data? There are a huge variety of tools available. Practitioners may lovingly hand draw images (physically or digitally), from early 19th century visualisations by Florence Nightingale and W. E. B. Du Bois to the work, verging on art, of Federica Fragapane. More commonly there are deeply customisable packages in modern programming languages like R, Python or Javascript, or commercial plotting software like Tableau. Excel has endured through its simple learning curve, ubiquity and reliable outputs. A whole separate suite of software exists for geospatial and linked data. Ultimately the right choice is decided by our use case, data, resources and skills.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Data Visualisation",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nExploratory data visualisations are both quite generic and very dataset specific. Simple plots like bar charts, histograms, scatter plots, and time series are simple enough to be reliable and fit the ethos of being quick and informative of the features of your data. This Power BI dashboard from Brandi Jagars at University of South Florida Libraries shows how simple visualisations give quick insights into a visitor dataset.\n\n\n\nA dashboard of exploratory graphs\n\n\nVisualisations designed for users and for publication are much more varied, and (usually) more polished. Maps are an eye-catching and engaging interface into collections that tap into users’ sense of place. The Mapping Manuscript Migrations project offers a map view of global manuscript migrations allowing users to track the movement of manuscripts filtered by things like collection, author and date of publication.\n\n\n\nMovement of manuscripts from the collection of Sir Thomas Phillipps\n\n\nPeripleo is a browser based map viewer that can be used with any cultural heritage dataset with associated location information. It was used in the Heritage for All project to displaying items in hyper-local contexts.\n\n\n\nMap of cultural heritage item locations around Exeter\n\n\nBoth of these initiatives rely on the concept of Linked Data. Each element of linked data is linked to other elements by one of a defined set of relationships, rather than the traditional spreadsheet model where each row is an item with a certain set of properties. This transforms the data into a network, which allows for intuitive, interactive visualisations that let users navigate material contextualised by the items closely linked to it.\nThe Semantic Name Authority Cymru (SNARC) provides views into a linked database of name authority records linked to Wales and the Welsh language. This makes graphs like this family tree of Charlotte Guest, a translator, businesswoman and noble, easy to produce. Displaying parts of the network let users understand the connections within it, as in this rather large, but very satisfying, graph of Welsh estates, houses and their owners.\n\n\n\nSNARC Welsh estates network graph\n\n\nThere’s also huge value in making visualisations like this available in physical form within the spaces of a library. The Bohemian Bookshelf was a 2011 project to increase serendipitous book discovery that was installed in the University of Calgary Library. It used 5 different visualisations to ‘entice curiosity’ by taking users to books and authors they might not have otherwise explored. This echoes Dr Mia Ridge’s proposed metric of ‘clicks to curiosity inspired’ in seeking ways to make it easy for users to be inspired by the collection. The ‘book pile’ arranged books by size and page count, acknowledging our natural fascination with the very large and the very small.\n\n\n\nThe Bohemian Bookshelf ‘Book Pile’\n\n\nYou can explore other use cases in this helpful list from the University of Minnesota Libraries. They’ve catalogued library specific resources for a range of use cases like the teaching, evaluation, and history of data visualisation in libraries.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Data Visualisation",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe best way to understand the value of data visualisations is to produce them for your datasets. Here are a few tools you can plug datasets into, organised by type and the skills needed to use them.\n\nImmediate results\nRAWGraphs is an online platform (no sign up required) you can upload a spreadsheet to (save it as a CSV first) or a JSON file and point and click your way down the page to produce a visualisation. It’s perfect for exploratory analysis and learning about the different kinds of visualisations available.\nVoyant Tools provides a similarly easy entry for corpus scale text data (whole works, or collections of works), though it helps if you know a little about corpus linguistics. There are ‘pre-built’ corpora of Shakespeare, Austen and Frankenstein available if you don’t have your own files to upload.\n\n\nIntro to coding\nThe R for Data Science Data Visualisation tutorial covers using ggplot2, the de facto standard for plotting in R. You can code along with an R environment in browser using Posit Cloud (requires a free account).\nSeaborn is one of the main plotting packages in Python and follows a similar philosophy to ggplot2. You can code along to their tutorial with a Python environment in browser using Google Colab (requires a Google account).\n\n\nSpreadsheet based\nExcel remains such an easy way to interact with spreadsheet data. If you haven’t used it before there are lots of resources available online or your institution may have Microsoft Office skills courses. This gentle intro from University of Minnesota libraries assumes some knowledge but not too much. PowerBI is a more advanced Microsoft Office app that allows you to create dashboards from data. It interfaces easily with Office software but the visualisations aren’t hugely inspiring. Google Charts and Sheets and OpenOffice Calc and Impress are equivalent alternatives.\nTableau is one of many commercial softwares for visualisation. There’s a learning curve similar to Excel, and a free (sign-up required) public platform you can try in browser, use the learning resources to get started.\n\n\nMapping and Linked Data\nGeospatial and linked data (where elements of the data can be explicitly linked to other elements) have their own worlds of tools, with functionality also often covered by the types of tools already listed. ArcGIS (now common in its online form) and QGIS are the most common paid and open source Geographic Information Software (GIS) tools available. You can use these to work with data with geographic components and make display worthy maps. Their outputs also plug into Python, R and JavaScript libraries like leaflet or Dash. For linked data tools like Gephi or Nodegoat have graphical user interfaces, or there are programming packages like igraph, NetworkX, and D3.js.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#recommended-readingviewing",
    "href": "dataviz.html#recommended-readingviewing",
    "title": "Data Visualisation",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMuch data visualisation is communication, and so is deeply subjective. Good visualisations are guided by design philosophies built upon how we process visual information. Two influential books that develop these philosophies are The Grammar of Graphics by Leland Wilkinson and The Visual Display of Quantitative Information (2nd Ed.) by Edward R. Tufte. The Grammar of Graphics is a hefty tome that proposes a core set of components for graphics then builds them from the ground up, with reference to programming. Visual Display is perhaps a little more accessible and uses the idea of how ink is used (digitally or physically) to understand what is and is not important in a graphic.\nIf visualisation is communication and how we understand visual communication is subjective, then catering generously to how different people process visual information is important. We might call this accessibility. This talk explains some of the basics of how our brains handle colour, and the importance of colour in visualisation. The Seaborn explanation of colour palettes is a helpful reference, and there is a free colour blindness tester to check that your visualisations encode information in colours that everyone can distinguish. Use alt-text for screen reader access and review for keyboard and content accessibility. Harvard have a helpful guide.\n\n\n\nThe importance of colour in recognising categories\n\n\nWith these considerations in mind there are graphics catalogues that help guide you towards the best visualisation for your purposes. from Data to Viz starts with types of data and leads you to appropriate graphs, while the Financial Times Visual Vocabulary starts with relationships between elements of your data and guides from there. Both are valuable.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#taking-the-next-step",
    "href": "dataviz.html#taking-the-next-step",
    "title": "Data Visualisation",
    "section": "Taking the next step",
    "text": "Taking the next step\nData visualisation, like any other skill, takes practice and familiarity with tools to get the best results. It is, however, relatively easy to make simple but effective visualisations that help you understand your data or explain it to someone else. So take some of your data and begin to play around with it. Play is an important word, you are being creative! Make mistakes and do the unexpected as you learn and you will be better for it. It is, unfortunately, just as easy to make bad visualisations that confuse you and your audience. Start simply, more complex visualisations will come with time.\nIf you haven’t done any visualisation before start with something like RAWGraphs or Excel for tabular data, or Voyant Tools for text data. If you have geographic data find a GIS specialist, or use the tutorials for QGIS or ArcGIS. If you have some experience with visualisations then tailor your choice of tool to your data. And if you’re familiar with programming then look at the plotting packages available, and enjoy the flexibility and reproducibility they give you.\nEngage with designers if they’re available to you and you are producing work for users or the wider public. The art of designing things for people applies as much to data visualisation as it does to anything else.\nAbove all remember your audience. Keep clear in your mind what it is you are trying to communicate, and to who, and ask yourself if your visualisation does that. Continue to iterate until it does, and you will have explained the story in your data clearly.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "getting-started-with-programming.html",
    "href": "getting-started-with-programming.html",
    "title": "Getting Started With Programming",
    "section": "",
    "text": "Introduction\naergergdfvdf",
    "crumbs": [
      "Getting Started With Programming"
    ]
  },
  {
    "objectID": "getting-started-with-programming.html#relevance-to-the-library-sector",
    "href": "getting-started-with-programming.html#relevance-to-the-library-sector",
    "title": "Getting Started With Programming",
    "section": "Relevance to the Library Sector",
    "text": "Relevance to the Library Sector\naergaergaerg",
    "crumbs": [
      "Getting Started With Programming"
    ]
  },
  {
    "objectID": "getting-started-with-programming.html#hands-on-exercises-self-guided-tutorials",
    "href": "getting-started-with-programming.html#hands-on-exercises-self-guided-tutorials",
    "title": "Getting Started With Programming",
    "section": "Hands-On Exercises & Self-Guided Tutorials",
    "text": "Hands-On Exercises & Self-Guided Tutorials\naergeargaerg",
    "crumbs": [
      "Getting Started With Programming"
    ]
  },
  {
    "objectID": "getting-started-with-programming.html#recommended-reading-further-resources",
    "href": "getting-started-with-programming.html#recommended-reading-further-resources",
    "title": "Getting Started With Programming",
    "section": "Recommended Reading & Further Resources",
    "text": "Recommended Reading & Further Resources\nragergeargaergaer",
    "crumbs": [
      "Getting Started With Programming"
    ]
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R",
    "section": "",
    "text": "All things R :)",
    "crumbs": [
      "Getting Started With Programming",
      "R"
    ]
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "All things Python :)",
    "crumbs": [
      "Getting Started With Programming",
      "Python"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Recommended Reading Lists",
    "section": "",
    "text": "There is no shortage of recommended reading lists out there, here we try and highlight a few of the most up to date and useful lists out there for the sector today!\nZotero Library for Digital Scholarship and Data Science Essentials for Library Professionals\nThis library contains links to all of the resources referenced within Topic Guides and across our site.\nZotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets\nOngoing collection of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data. This group is an open resource, welcoming contributions from anyone who has a resource to share.\n[TODO] Data Science in Libraries, is there a Zotero?",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Recommended Reading Lists"
    ]
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "Skills Competency Frameworks & Key Reports",
    "section": "",
    "text": "The following are Skills Competency Frameworks & Key Reports relating to supporting Digital Scholarship and Data Science for library staff\n\nLIBER Publications\n\nLIBER Digital Skills for Library Staff & Researchers Working Group - LIBER Europe have lots of resources here, including a very useful diagram Identifying Open Science Skills for Library Staff & Researchers\nLIBER Job Description Repository Contains job description examples for Digital Curator and other digital roles which reference the types of skills required for such work.\nEurope’s Digital Humanities Landscape: A Study From LIBER’s Digital Humanities & Digital Cultural Heritage Working Group is a report based on a Europe-wide survey run by LIBER’s Digital Humanities & Digital Cultural Heritage Working Group. The survey focused on digital collections and the activities libraries undertake around them. It covered the following topics and themes including staffing/skills\n\n\n\nKey Publications specific to digital scholarship and data science skills for research library staff\n\nThe British Library and the Arts and Humanities Research Council published a report on skills: Scoping Skills and Developing Training Programme for Managing Repository Services in Cultural Heritage Organisations. There is a very useful section (Section 3.) that references several other digital skills frameworks for research library staff across Europe.\nLippincott, Joan K. Directions in Digital Scholarship: Support for Digital, Data-Intensive, and Computational Research in Academic Libraries. Coalition for Networked Information, June 2023. https://doi.org/10.56561/ULHJ1168\nPadilla, Thomas. ‘Responsible Operations: Data Science, Machine Learning, and AI in Libraries’. OCLC, 26 August 2020. https://www.oclc.org/research/publications/2019/oclcresearch-responsible-operations-data-science-machine-learning-ai.html.\nCordell, R. C. (2020). Machine Learning + Libraries: A Report on the State of the Field. LC Labs, Library of Congress. https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf\nFederer L. Defining data librarianship: a survey of competencies, skills, and training. J Med Libr Assoc. 2018 Jul;106(3):294-303. doi: 10.5195/jmla.2018.306. Epub 2018 Jul 1. PMID: 29962907; PMCID: PMC6013124.\n\n\n\nGeneral Competencies for Librarians which include reference to digital\n\nAmerican Library Association (ALA) Library Competencies (Various roles): Library Competencies | Tools, Publications & Resources (ala.org) (USA)\nCanadian Association of Research Libraries Competencies for Librarians in Canadian Research Libraries Publications and Documents (including specifically Competencies-Final-EN-1-2.pdf (Canada)\nCILIP: the library and information association Professional Knowledge & Skills Base - (UK)",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Skills Competency Frameworks & Key Reports"
    ]
  },
  {
    "objectID": "training-platforms.html",
    "href": "training-platforms.html",
    "title": "Training Platforms",
    "section": "",
    "text": "When you’re ready to go further and have a better idea of the specific skills you need for a particular task, we can recommend having a good search through these excellent platforms which host a great many in-depth training materials:\n\nDARIAH-Campus\nDARIAH is a pan-European infrastructure for arts and humanities scholars working with computational methods. It supports digital research as well as the teaching of digital research methods. Though not specific to the library professional context, tutorials here are useful for applying techniques to digital collections. https://campus.dariah.eu/\n\n\nThe Glam Workbench\nThe GLAM Workbench is the brainchild of Tim Sherratt, a historian, and is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions (galleries, libraries, archives, and museums). It includes tools, tutorials, examples, hacks, and even some pre-harvested datasets. It’s aimed at researchers in the humanities but has useful tutorials for anyone interested in working with GLAM data. https://glam-workbench.net/\n\n\nIneo\nIneo is a project developed and maintained by CLARIAH that lets you search, browse, find and select digital resources for research in humanities and social sciences. At the end of 2024 it will offer access to thousands of tools, datasets, workflows, standards and learning material. It is a work in progress so do keep that in mind when browsing. https://www.ineo.tools/\n\n\nLibrary Carpentry\nLibrary Carpentry is an international volunteer community, under the Carpentries, focussed building software and data skills within library and information-related communities. The lessons here are meant to be taught as workshops led by a Carpentries certified instructor (for a fee) but you may find it useful to have a read through the content which is open and available to all. https://librarycarpentry.org/\n\n\nThe Programming Historian\nThe Programming Historian has been publishing peer-reviewed tutorials on digital tools and techniques for humanists since 2008 and though they’re generally aimed at academic researchers, staff at British Library have found them highly useful over the years in their own work! https://programminghistorian.org/en/",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Training Platforms"
    ]
  },
  {
    "objectID": "networks.html",
    "href": "networks.html",
    "title": "Useful Networks",
    "section": "",
    "text": "LIBER Working Groups\nWorking groups are open to staff at participating LIBER Member institutions: - LIBER Data Science in Libraries - LIBER Digital Scholarship & Digital Cultural Heritage - Or have a look at the other LIBER Working Groups - LIBER Europe",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  },
  {
    "objectID": "networks.html#international-networks",
    "href": "networks.html#international-networks",
    "title": "Useful Networks",
    "section": "International Networks",
    "text": "International Networks\n\nAI4LAM\nCode4Lib\nElectronic Literature Organisation\nFlickr Commons\nGlam Labs International\nIIIF/UV Open Collective\nIMPACT Centre of Competence\nMuseums Computer Group\nTranskribus\nWikidata Community)",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  },
  {
    "objectID": "networks.html#national-networks-european",
    "href": "networks.html#national-networks-european",
    "title": "Useful Networks",
    "section": "National Networks (European)",
    "text": "National Networks (European)\n\nIreland/UK\n\nRLUK Digital Scholarship Network",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  }
]