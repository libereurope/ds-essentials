[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "",
    "text": "Welcome\nDigital Scholarship & Data Science Essentials for Library Professionals (DS Essentials) is an open and collaboratively curated training reference resource. It aims to make it easier for LIBER library professionals to gain a concise overview of the new technologies that underpin digital scholarship and data science practice in research libraries today, and find trusted training materials recommendations to start their professional learning journey.\nThe project began in late 2023 as a joint collaboration between the Digital Scholarship & Digital Cultural Heritage (DSDCH) and the Data Science in Libraries (DSLib) working groups of LIBER. Work is currently underway to create a first edition ready for the LIBER Winter Event 2024. Contributions to this work are very much welcome!",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#our-aims",
    "href": "index.html#our-aims",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "Our Aims",
    "text": "Our Aims\n\nPresent a central destination for newcomers to become more familiar and conversant with the foundational concepts, methods and tools of digital scholarship and data science practice in libraries. Examplar topics include:\nHelp library staff gain a better understanding of digital scholarship and data science practice in libraries by providing contextualised and gentle introductions to key topics through guides, self-guided training materials, project case studies, jargon busting and recommended reading lists\nEnable staff, in LIBER membership and beyond, to explore and immediately access a wealth of self and group study educational resources, available to them through a curated catalogue with faceted search and discovery\nFoster a culture of continuous learning and professional development and facilitate the development of localised training opportunities in LIBER institutions by providing guidance and training materials suitable for self and group study\nOffer a centralised, curated and trusted home for educators to contribute, and have reused, their high-quality training materials\nRaise awareness of local, national and international networks, and associated training events, relevant to delivering state of the art services in libraries\nFurnish current reports and competency skills frameworks pertaining to digital scholarship and data science in libraries",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#who-is-it-for",
    "href": "index.html#who-is-it-for",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "Who is it for?",
    "text": "Who is it for?\nAre you someone working in or around research libraries with an interest in learning more about how to do cool and interesting things with digital collections and data at your institution? Wondering how data science techniques can help you in your work? Are you interested in gaining valuable digital literacy skills and knowledge to support emerging areas of modern scholarship such as Digital Humanities? Do you need some of the technology jargon you hear about these days demystified?\nThen this resource is for you!\nIt is very important to us that this resource is inclusive and intellectually accessible, challenging but not terrifying and as such we focus primarily on an introductory audience where no programming or particular digital skills are required.\nThough written primarily from the research library professional perspective these guides will be useful for anyone currently (or aspiring to be) working in and around digital collections and data in heritage institutions:\n\nLibrary & Information Science students\nProject managers\nDevelopers\nInformation specialists\nMetadata Managers\nSubject librarians\nSystem librarians\nInstitutional leadership\n\nAnd so many more!",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#why-this-resource",
    "href": "index.html#why-this-resource",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "Why this resource?",
    "text": "Why this resource?\nOver the past few decades, the development of excellent self-paced tutorials and training materials relating to undertaking digital scholarship and data science in libraries have proliferated online.\nFor library professionals who are relatively new to this area however, it can be hard to know where to begin! Without knowing a little bit about the context of how new technologies are being deployed in research libraries, this dearth of resources can seem quite daunting. Even if you might have an idea of what learning you’d like to undertake, it can be difficult and time consuming to try and navigate the wealth of individual training resources out there on your own.\nDigital Scholarship & Data Science Essentials for Library Professionals is a training reference resource we have developed to remove some of the barrier of having to hunt for recommended training and resources to get you on the learning ladder.\nFrom working with data, data management, digital storytelling, handwritten recognition technologies and more, our Topic Guides provide contextualised and gentle introductions, written from the library practitioners perspective, to a wide range of key topics and technologies relevant to working innovatively with our digital collections and data.",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#what-are-topic-guides",
    "href": "index.html#what-are-topic-guides",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "What are Topic Guides?",
    "text": "What are Topic Guides?\nTopic Guides are the heart and soul of Digital Scholarship & Data Science Essentials for Library Professionals. The aim of each is to provide research library professionals with a gentle and concise introduction to the key topics in this area today. They offer those with no prior knowledge, quick and curated guidance to personally recommended hands-on tutorials, use-cases, articles, videos, networks and communities of practice to deepen learning.\nEach Topic Guide follows a fixed structure, consisting of five components in order to make it easier to quickly find what you might need:\n\nIntroduction to the topic\nRelevance to the library sector (Case studies/Use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended Reading & Viewing\nTaking the next steps (Communities of Practice)",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-resource",
    "href": "index.html#how-to-use-this-resource",
    "title": "Digital Scholarship & Data Science Essentials for Library Professionals",
    "section": "How to use this resource",
    "text": "How to use this resource\nThis resource has a dual purpose; it is written as a tool for self-study but also as a guide for individuals and institutions interested in establishing their own local training programmes.\nAs a self-study resource, we hope it can serve as a useful first point of entry into complex topics in digital scholarship and data science and their application in the library world. It is not meant to be completed in any particular order, rather, learners are invited to jump in and out of individual topic guides as personal curiosity or practical need dictates.\nThe materials here can also be utilised in group study, as part of a reading group or even a hands-on Hack & Yack. See our special section on “How to start your own training programme” for some tips and tricks from British Library Digital Research Team colleagues on utilising the materials referenced here at your own library or within your network.",
    "crumbs": [
      "**ABOUT**",
      "Welcome"
    ]
  },
  {
    "objectID": "project-overview.html",
    "href": "project-overview.html",
    "title": "Project Background",
    "section": "",
    "text": "Core Project Team\nThe site and content are maintained by the Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nNeha Moopen, Site Maintainer DSLib\nPeter Verhaar, Editor DSLib",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "project-overview.html#community-contributors",
    "href": "project-overview.html#community-contributors",
    "title": "Project Background",
    "section": "Community Contributors",
    "text": "Community Contributors\nEach Topic Guide is written by specific named contributors but we also welcome changes and contributions to this resource via logging issues or if you’re a more seasoned GitHub user, via pull requests to the github repository. A full list of contributors will be compiled here once the first edition is complete.",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "project-overview.html#acknowledgements",
    "href": "project-overview.html#acknowledgements",
    "title": "Project Background",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe project team would like to thank all of our LIBER working group members who have contributed to this resource and supported our endeavours here. We have taken great inspiration from other incredible training initiatives such as the British Library Digital Scholarship Training Programme, DH Literacy Guidebook and The Programming Historian in the development of this resource and thank those projects for paving the way!",
    "crumbs": [
      "**ABOUT**",
      "Project Background"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contribute to Our Project",
    "section": "",
    "text": "Ways to contribute to this project",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#ways-to-contribute-to-this-project",
    "href": "contributing.html#ways-to-contribute-to-this-project",
    "title": "Contribute to Our Project",
    "section": "",
    "text": "Contribute to an existing, or create a new Topic Guide (individually or in collaboration with others). Choose from our existing wishlist or propose a new one. Join an upcoming writing sprint or get in touch with us directly to discuss digitalresearch@bl.uk your idea and we’ll help you get started right away!\n\n\nNOTE: Topic Guides we are seeking authors for right now include:\n\nAutomatic Text Transcription (OCR/HTR)\nDemystifying Computational Environments for Digital Scholarship\nClimate Change and Sustainability\nCultural Competency & Ethics in Digital Methods\nDigital Scholarship/Data Science Project Management, Evaluation and Impact Assessment\nFoundation models: When and why to use LLMs or SLMs\n\n\n\nSuggest edits to an existing Topic Guide or any of the General Resources pages by opening a new Issue or adding to the discussion on existing Issues on the project Github. If you’re new to GitHub don’t worry, we have a Topic Guide for that: GitHub: How to navigate and contribute to Git-based projects!\nJoin one of the LIBER Working Groups Digital Scholarship and Digital Cultural Heritage WG or Data Science in Libraries (open to all staff of LIBER member institutions) to keep up with the developments and ongoing maintenance of this project!",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#join-a-writing-sprint",
    "href": "contributing.html#join-a-writing-sprint",
    "title": "Contribute to Our Project",
    "section": "Join a Writing Sprint!",
    "text": "Join a Writing Sprint!\n\nOur first Topic Guide Sprint was held online Tuesday 14 May 2024 and another Tuesday 03 June 2024. To be notified of upcoming sprints check back here for dates/times or join one of our working groups and we’ll email out the details!\n\n\nPreparing for a sprint:\n\nPlease have a quick read through the Welcome and Topic Guide pages to familiarise yourself with this resource and its purpose.\nWhen you Register you’ll see a list of our latest Topic Guides on our wishlist. Please check off any and all Topic Guides you would be happy to work on during the day of the sprint. On the day each attendee will focus on writing just one Topic Guide however. The information you provide before the sprint will help us organise this ahead of time so that as many Topic Guides as possible have at least one dedicated author.\n\nPlease have a quick read through the Author Guidance and Style Guide section.\nWe’ll be in touch after we receive your registration to send you more details and useful information ahead of the sprint so look out for our email! If you are sent a copy of the Topic Guide Template (Google Doc) that corresponds to your topic of choice ahead of time, add your name to the header, and feel free to add notes or even make a start on drafting your guide before the sprint day if you’re keen!\n\n\n\nWhat to expect on the day:\nA two hour writing sprint will generally follow this format:\n\nWelcome and participant introductions (10 minutes)\n\nOverview of the project & explanation of how the day will run (15 minutes)\n\nAllocation and confirmation of individual Topic Guide Authorship for the day (15 minutes): As a group we’ll go over the wishlist and all participants will be given access to their particular Topic Guide google doc template they’ll be using during and after the sprint.\nWriting Phase (60 minutes): You’ll have an hour to begin writing your respective guide in your Google Doc template. If more than one participant are working together on a topic we can provide breakout rooms so that you can discuss amongst yourselves how to break up the work, and share ideas as you go along. You can have your camera/mic on or off during this time and co-chairs will be on hand to answer any questions or give advice on topics.\nWrapping Up & Logging progress through Github Issues (15 minutes): We have provided an Issue Template which is pre-formatted as a Topic Guide submission checklist and will walk participants through the process of opening a new issue for your draft submission. Please fill in the issue template and provide the necessary information relating to where your draft submission is currently at. Once the issue is saved, it can continue to serve as a space to continue discussion of the ongoing status of your submission and log updates where necessary, including notifying us of when it is fully ready for final review by maintainers.\nClosing Remarks & Next Steps (5 minutes)\n\n\n\nAfter the sprint:\nDuring the sprint, you’ve drafted your Topic Guide in a Google Doc. This Google Doc can continue to serve as a ‘living document’ following the sprint until you feel the content is ready to be published. Throughout this process, we will use your Github issue to maintain an overview of the docs in terms of their status and action points so please keep this updated. Each Topic Guide will be allocated a specific Maintainer contact (one of the co-chairs) who will be in touch and work with you to see the process through to completion. You may request placing your Google Doc under restricted access if you wish. We will not do this by default, but we respect that some contributors would like to keep drafts private until fully ready for submission.\nWhen your Topic Guide is ready for our review in Google Docs, please let us know by logging an update over on Github as a comment on the Issue stating this. You can also tag the maintainers so they receive a notification as well. Maintainers will generally work with you on the final edits necessary for the submission through track changes and comments within the Google Doc itself. Once these have all been resolved maintainers will log a status update in the GitHub issue.",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "contributing.html#ongoing-maintenance-and-review",
    "href": "contributing.html#ongoing-maintenance-and-review",
    "title": "Contribute to Our Project",
    "section": "Ongoing Maintenance and Review",
    "text": "Ongoing Maintenance and Review\nWe view this resource as capturing a snapshot in time and will endeavour to fully review all content a minimum of once annually to check for link rot, and content relevancy. The project team editors are committed to make quick fixes when raised throughout the year, while formal sprints and reviews coinciding with LIBER Annual Summer Conference and Winter events will be used for soliciting new content and undertaking more complex updates to the resource each year and creating new editions as necessary.\nJoin one of the LIBER Working Groups Digital Scholarship and Digital Cultural Heritage WG or Data Science in Libraries (open to all staff of LIBER member institutions) to keep up with the developments and ongoing maintenance of this project!",
    "crumbs": [
      "**ABOUT**",
      "Contribute to Our Project"
    ]
  },
  {
    "objectID": "guidelines.html",
    "href": "guidelines.html",
    "title": "Author Guidance and Style Guide",
    "section": "",
    "text": "Structuring the Topic Guide\nEach Topic Guide follows a distinct structure, beginning with a Header Section composing of a title, contributor information, published date, last modified info, and suggested citation followed by five key content blocks detailed below. Please have a look at the IIIF Guide as an example of how these pieces all fit together into one complete Topic Guide.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "guidelines.html#structuring-the-topic-guide",
    "href": "guidelines.html#structuring-the-topic-guide",
    "title": "Author Guidance and Style Guide",
    "section": "",
    "text": "Header Section\n\nTopic Guide Title: IIIF\nContributor(s) Name and (Orcid ID): Nora McGregor (ORCID iD)\nPublished date: 27/03/2024\nSuggested Citation: Nora McGregor, “IIIF,” Digital Scholarship & Data Science Essentials for Library Professionals (2024), [DOI link]\n\nNote: The project team will ensure each Topic Guide is given an individual DOI either through British Library Research Repository or Zenodo.\n\n\nI: Introduction to the topic (Approximately 500-700 words)\nThis introduction section should be pitched at a beginner/foundational level and gives a concise overview of the topic. It should be written in a relaxed and natural way. This section does not need to contain the world’s knowledge, just enough high-level knowledge to get the key concepts across.\n\nImagine that a colleague has come to you casually asking about the topic over tea. How might you go about explaining it to them in your own words? During the course of that casual conversation what key things would you leave in and what might you leave out in the interest of getting them to a basic understanding quickly?\n\nLinking can be used liberally for jargon busting throughout if explaining a particular term is more complex than our word count allows. Please see the Style Guidelines below for more writing tips!\n\n\nII: Relevance to the Library Sector (Approximately 300-500 words)\nThis section provides a clear explanation of the topics’ specific relevance to the work of libraries.\nIt should contain:\n\nA short paragraph or two setting the scene as to why the topic is relevant to the work of libraries. Here you might also like to present opportunities and, if relevant, potential challenges for libraries around the topic as well.\nUp to 3 examples of real world (or potential) applications/case studies/projects briefly explained in a 100 max word summary provided each with links to further information if available. Note that it is not necessary to write up or create a new case study yourself here! Rather, we’re looking for briefly summarised references to existing ones. If there are quite a few other examples the contributor would like to reference include, links to these can be added at the end of this section (Example: “For further case studies, visit….”).\n\n\n\nIII: Hands-on activity and other self-guided tutorial(s)\nThe objective of this section of the topic guide is to enable learners to familiarise themselves with the basics of the topic through active practice via self-paced tutorials and hand-on activities. Authors are not expected to create new activities or tutorials in this section, but rather to provide selected links to existing hands on tutorials which are known by them to have proven value and can be personally recommended for library professionals in particular.\nThe tutorials recommended should be free, online and suitable for independent study, and ideally, focussed on the library professional perspective where possible. Authors may wish to link to practical exercises or quizzes, specific online lessons that may exist in other online platforms, Juptyer Notebooks and GLAM workbench materials that provide detailed explanations of the steps learners can follow.\nFor the sake of consistency across the various topic guides, it is helpful if tutorial references are structured as follows:\n\nThe title/name of the activity/tutorial, the URL (added as a hyperlink), and citation information (please use APA Style and/or provide a DOI if there is one)\nA brief, personal explanation below it (no more than 200 words) as to why the author recommends this particular tutorial, and, optionally, an indication of topics covered and the level of complexity.\n\n\n\nIV: Recommended Reading & Viewing (Approximately 200-400 words)\nThe section on recommended reading and viewing contains references to more passive learning resources such as:\n\nOpen access articles discussing the topic at a general level, or containing contextual information.\nVideo recordings of lectures about the topic, which do not demand practical activities from the viewer Podcasts about the topic\n\nWhen including citations please make sure to either write them in APA Style and/or simply provide the DOI with your text so that the project team can compile a dedicated Zotero Library.\n\n\nV: Taking the next steps (Approximately 200-400 words)\nThis section provides guidance to library professionals on where to take their learning journey further. It should include:\n\nWhere to find (local/national/international) Communities of Practice or other relevant networks and organisations who can help with furthering their understanding of the topic.\nIf relevant you might also point to specific summer school courses, conferences and events that may further enhance learning.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "guidelines.html#style-guidelines",
    "href": "guidelines.html#style-guidelines",
    "title": "Author Guidance and Style Guide",
    "section": "Style Guidelines",
    "text": "Style Guidelines\n\nWriting Accessible, Natural, and Internationally Inclusive Content\nTo ensure consistency and inclusivity across our content, please consider these general guidelines:\n\nClarity and Simplicity: Write in a clear and straightforward manner, using simple language that is easy to understand for learners of all backgrounds and proficiency levels.\nLinking to Technical Terms: When introducing technical terms or concepts that may be unfamiliar to some learners, provide hyperlinks to additional resources or definitions where they can learn more. This helps to enhance understanding and allows learners to explore topics in more depth at their own pace. Ensure that the linked resources are reliable and authoritative to provide accurate information to the learners.\nAvoid Colloquialisms and Regionalisms: While it’s essential to maintain a casual and natural tone, please refrain from using colloquial expressions or regionalisms that may not be universally understood by our diverse audience.\nCultural Sensitivity: Be mindful of cultural differences and avoid language or examples that may be offensive or insensitive to any group of people. When providing examples or references, strive for universality and inclusivity.\nGender Neutrality: Use gender-neutral language whenever possible to ensure inclusivity and avoid assumptions about gender roles or identities.\nGlobal Perspective: Consider the international nature of our audience when crafting examples, scenarios, and references. Aim for content that resonates with learners from various cultural backgrounds and geographical locations.",
    "crumbs": [
      "**ABOUT**",
      "Author Guidance and Style Guide"
    ]
  },
  {
    "objectID": "licensing.html",
    "href": "licensing.html",
    "title": "Licensing & Re-use",
    "section": "",
    "text": "Citing this Resource\nSuggested citations are written in APA (American Psychological Association) citation style for Journal Articles and are given on each individual guide:\nMcGregor, Nora, “IIIF,” Digital Scholarship & Data Science Essentials for Library Professionals (2024), [DOI link]\nTo cite the full resource:\nMcGregor, N., Verhaar, P., Moopen, N., Double, J., Irollo, A., & Kiraly, P. (Eds.). (2024, March 12). Digital Scholarship & Data Science Essentials for Library Professionals. https://libereurope.github.io/ds-essentials",
    "crumbs": [
      "**ABOUT**",
      "Licensing & Re-use"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "If you’d like to ask a question of the project team you can please send an email to Nora McGregor, nora.mcgregor@bl.uk or any of Co-Chairs and select Members of the collaborating WGs listed here who act as the core project delivery team and editors of this resource.\nJodie Double, Editor DSDCH\nPéter Király, Editor DSLib\nNora McGregor, Editor DSDCH\nNeha Moopen, Site Maintainer DSLib\nPeter Verhaar, Editor DSLib",
    "crumbs": [
      "**ABOUT**",
      "Contact"
    ]
  },
  {
    "objectID": "topicguides.html",
    "href": "topicguides.html",
    "title": "TOPIC GUIDES",
    "section": "",
    "text": "Topic Guides are the heart and soul of Digital Scholarship and Data Science Essentials for Library Professionals. The aim of each is to provide research library professionals with a gentle and concise introduction to the key topics in this area today. They offer those with no prior knowledge, quick and curated guidance to personally recommended hands-on tutorials, use-cases, articles, videos, networks and communities of practice to deepen learning.\nEach Topic Guide follows a fixed structure, consisting of a descriptive header and five key components in order to make it easier to quickly find the information you might need:\n\nIntroduction to the topic\nRelevance to the library sector (case studies/use cases)\nHands-on activities and other self-guided tutorial(s)\nRecommended reading & viewing\nTaking the next steps (finding Communities of Practice)\n\nThe guides are not presented, nor meant to be completed, in any particular order, rather, you are invited to jump in and out of individual topics as personal curiosity or practical need dictates.\nThe Topic Guide list in this first edition draws heavily on the existing skills framework and topics covered in the British Library’s Digital Scholarship Training Programme, as well as those recommended by attendees of a series of development workshops held with LIBER working groups throughout 2023.\nOur current wishlist includes but is not limited to topics such as:\n\n2D/3D Imaging: 3D Modelling and RTI\nAutomatic Text Transcription (OCR/HTR)\nDemystifying Computational Environments for Digital Scholarship\nClimate Change and Sustainability\nCollections as Data\nCopyright and Licensing\nCrowdsourcing & Citizen Science in the cultural heritage context\nComputer Vision\nCultural Competency & Ethics\nData Visualisation\nDigital Mapping\nDigital Scholarship Project Management, Evaluation and Assessment\nDigital Storytelling\nGetting started in programming\nGithub and GitPages/Quarto\nIIIF\nLinked Open Data\nMachine Learning and AI in Libraries Literacies\nOpen Research\nResearch Data Management\nWhat is an API?\nWikimedia\nWorking with Data",
    "crumbs": [
      "**TOPIC GUIDES**"
    ]
  },
  {
    "objectID": "ml-ai.html",
    "href": "ml-ai.html",
    "title": "AI & ML in Libraries Literacies",
    "section": "",
    "text": "Introduction: AI & ML terms demystified\nAI is mentioned absolutely everywhere these days, first it was just in movies, the news, but now it’s cropping up in our library meetings and strategies and funding calls, but what does it really mean, particularly in a library context? Let’s try to get to the bottom of this!\nTo do that I always like to start off with a bit of basic jargon busting.\nArtificial Intelligence (AI) is actually a really broad field of computer science (and an umbrella term) that refers to the research and development of systems and machines capable of doing tasks that typically require human intelligence to perform, such as\nSometimes folks may speak of or refer to AI as systems and machines that actually have true intelligence, and though today’s AI systems are shockingly convincing in how well they perform, what we’re seeing today are just very advanced machine learning algorithms and models performing specific and discrete functions extremely well! We’re a long way off (if ever) from machines having sentience (or, Generalised Artificial Intelligence (GAI)/Strong AI) so don’t worry!\nYou might also sometimes hear people talk about Traditional AI vs Generative AI. Traditional AI refers to using machine learning based systems for doing tasks like classifying data (e.g., assigning labels to images, automatically transcribing handwritten texts, or identifying genre of digitised texts). This is the type of AI we make a whole lot of use of in the library world. Generative AI on the other hand refers broadly to systems whose primary function is to generate new content (e.g., conversation, books, art). This is where conversation generating AI systems like ChatGPT (Generative Pre-trained Transformer) fall under for example and we’re only just now exploring the potential applications for these new powerful Generative AI systems in library work.\nWhenever AI is being discussed you may often hear the term Machine Learning (ML) mentioned, and sometimes they’re used interchangeably which can be confusing!\nMachine Learning (ML) is more specifically a main subfield of AI and core technology underpinning the other subfields of AI, that focuses on the development of algorithms and models that allow computers to learn patterns and relationships from data and make predictions on new data. Instead of being explicitly programmed for specific tasks, ML algorithms use data to learn and improve their performance over time.\nAn algorithm is a plan, a set of step-by-step instructions, in order of operation, for solving a problem or performing a task. Making a sandwich is a classic example. “Get two pieces of bread…” If we want to tell a computer to do something, we have to write a computer program that will tell it, step-by-step, exactly what we want it to do and how we want it to do it.\nA machine learning algorithm is a special kind of algorithm designed to help computers learn from data. Instead of giving the computer explicit instructions for every task, we give it data and let it find patterns, relationships, and trends in that data in order to make decisions or predictions on new data on its own.\n“Training a model” is the process of teaching a machine learning algorithm to make predictions or decisions based on data. It’s important to remember that data is the lifeblood of ML and the model is only as good as the type and quality of data you give it.\nA machine learning model represents what was learned by a machine learning algorithm and this is what can be used to make predictions on new data without needing explicit instructions. The model contains the rules, numbers, and any other algorithm-specific data structures required to make predictions on new data. If it doesn’t work very well, you can go back and give the algorithm more data or tweak its parameters to create a new better model.\nWe actually do quite alot of the above at the British Library, you can read more about the actual process and results here, and I’ll cover a bit more of this in the next section!\nSnapshot of an interface from Transkribus software used to create data to train a model to recognise handwritten arabic manuscript pages from a British Library collection.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#introduction-ai-ml-terms-demystified",
    "href": "ml-ai.html#introduction-ai-ml-terms-demystified",
    "title": "AI & ML in Libraries Literacies",
    "section": "",
    "text": "Reasoning\nProblem-solving\nLearning\nPerception\n\n\n\n\n\n\nThe primary task of Machine Learning is prediction.\n\n\n\n\n\n\nHere’s a super simplistic example of this type of process in action:  Let’s say I have thousands of images of handwritten manuscript pages digitised from our library collection and ready to go online. The problem is, I want to make them all text searchable as well, but to transcribe these all by hand would take me well into my retirement and I have other things to be getting on with. I would like to train a machine learning model to help me automatically recognise the handwritten text in this collection. I would first need to somehow show my machine learning algorithm examples of a correct result so that it could start to recognise the pattern of that (in this case, text on a page). To do this I can show it images with associated text annotations which have been transcribed perfectly by me, the more, the better! Once the algorithm has learned a bit about what a correct page of handwritten text looks like, a model is then created which contains all the rules and parameters and calculations ready for the particular task I have set it of predicting what handwritten text is on pages of this particular collection that it’s never seen before!\n\n\n\n\nOk, quick quiz time!\nRemembering that Machine Learning is about prediction, which of the following do you think would require ML?\n\nCounting the number of people in a museum using information from entry and exit barriers.\nA search system that looks for images similar to a user submitted sketch.\nA system that recommends library books based on what other users have ordered.\nA queueing system that spreads people evenly between 5 ticket booths\nA program which extracts names from documents by finding all capitalised words and checking them against a list of known names\nA system which turns digitised handwritten documents into searchable text\nA robot that cleans the vases in a museum without bumping into them or breaking them\n\nIf you answered 2, 3, 6 & 7 you are correct! The others could all be most easily executed with a straightforward algorithm, programmed using a simple set of easily defined rules, rather than requiring prediction.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "ml-ai.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "AI & ML in Libraries Literacies",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nAs you now know, machine learning algorithms and models underpin all the other subfields of AI and there are a LOT of subfields of AI depending on who you talk to! In this guide though we’ll focus our attention on just these two particular AI research areas to give you a general sense of how machine learning is practically applied in the library context today:\n\nNatural Language Processing (NLP): which is concerned with making AI systems more capable of natural and effective interaction with humans (text and speech)\nComputer Vision (CV): which is concerned with enabling machines to interpret and make decisions based on visual data from the world.\n\nLet’s have a look at these through some use cases:\n\nNatural Language Processing (NLP)\nNLP involves the development of a wide range of algorithms, models, and systems for analysing, understanding and extracting meaningful information from textual and speech data representing human language. We can use NLP for things like:\n\nSubject indexing to enhance library catalogue search\nNamed Entity Recognition (NER) is a text analysis process within NLP that helps turn unstructured text into structured text. A sentence or a chunk of text is parsed through to find entities that can be put under categories like names, organisations, locations, quantities, monetary values, percentages, etc.\n\nIn the library world it can be used as part of a process to understand what subjects (people, places, concepts) are contained within a digitised text and help us enhance our catalogue records for items or search functionality. There is a very nicely outlined use case here of how the United States Holocaust Memorial Museum used NER to automatically extract person names and location from Oral History Transcript to improve indexing and search in their catalogue.\n\n\nAutomatic Language Detection & Genre Classification\nThe British Library has used different machine learning techniques and experiments derived from the field of NLP to assign language codes and genre classification to catalogue records, in order to enhance resources described. In the first phase of the Automated Language Identification of Bibliographic Resources project, language codes were assigned to 1.15 million records with 99.7% confidence. The automated language identification tools developed will be used to contribute to future enhancement of over 4 million legacy records. The genre classification case study includes a nice description of machine learning as well as references to other use cases for metadata clean up.\n\n\nText & Conversation Generation\nLanguage models are a type of machine learning model designed to predict the likelihood of a sequence of text, which means that they can be set up to predict the most likely way to continue a conversation. A large language model (LLM), such as the models behind ChatGPT, are highly complex neural networks that have been exposed to an enormous amount of text from books, articles, websites, and more. They perform natural language processing tasks such as generating and classifying text, answering questions, and translating text and are the backbone of NLP today. On the other hand there are small language models (SML) too which, well, you guessed it, are much smaller, as in, they don’t need quite as much data and time to be trained. Whether or not to use either depends on your use case and motivations!\nThere are and have been for many years, large language models out there actually but ChatGPT has currently caught the popular imagination because of its publicly available interface and remarkable performance, so it’s worth spending a little time here unpacking just what ChatGPT is and its potential impact on library work.\nThe large language models behind ChatGPT have learned something about patterns in grammar and word meaning, including the way that meaning arises contextually across multiple sentences and multiple turns in a conversation. When you ask ChatGPT a question, you are presenting the model with new information it tries to make a prediction on, in this case, it tries to generate a response that matches the pattern of conversation.\nYou ask questions or give prompts to the model, and it provides responses in natural language, or rather, estimates what should come next in a conversation. When ChatGPT gives a response, it isn’t actually looking up information and then composing that information into a response; it’s just making an estimation of a response based on patterns it has seen. So, when you ask it factual questions, especially ones with common answers or phrases, it might give you an answer that sounds right but remember this is because it’s mimicking what it has seen in its training data.\nLibrarians are still investigating use cases for this new Generative AI applications, but for now at least, ChatGPT is certainly useful as a personal writing assistant or tool to help give you ideas for\n\ncreating a title for a new exhibition\ncreating exhibition labels\noutlining a basic structure for an information literacy workshop\ncreating a blog post on a topic for which you are very familiar\nhelping you reword something for different audiences\nwriting a funding proposal!\n\nIt’s also good to get in the habit of trying out and being aware of how these particular models work as more and more library users will be using this technology too, and may not know quite have a clear understanding of what’s behind the responses generated by them. We’ve seen librarians having to answer queries about citations that have been made up by ChatGPT, article references which sound very much like they exist, but have just been hallucinated by the model!\n\n\n\nComputer Vision (CV) use cases\nWe can use computer vision to train models to automatically analyse and understand useful information from images and videos.\nIn the library world we can use this to label millions of images with descriptive metadata (“this is a picture of a cat”), or, as we see below, a model can be trained to classify this image as a newspaper based on objects identified in the layout (for example, a nameplate for the newspaper, a headline, photographs, and illustrations and so on). The model learns how to identify that this is the NYT based on learning from other newspaper images it’s seen (for example, if given NY Tribune, NY Times, and NY Post images, it can distinguish between the various titles).\n\n\n\nPutting it altogether: ML + CV + NLP\nOne of the state of the art applications of machine learning seen in cultural heritage at the moment is Handwritten Text Recognition (HTR) which we had a look at earlier briefly in our simple example of training a model. The idea with HTR is to convert digitised handwritten documents into searchable and machine readable text. To achieve this HTR actually uses a combination of Computer Vision (CV) and Natural Language Processing (NLP).\nSince handwriting can be tricky and ambiguous you might have a Computer Vision model try to identify possible letters from the shapes, and another to work out what the most likely word is from those shapes. But let’s imagine that there’s a smudge on the page, and the letters and maybe even whole words are completely illegible. In that case you might turn to your NLP language models which look at sentence level predictions, taking into account words in the whole line of text the model uses that context to work out what words are most likely missing in those smudged spots!\nSometimes a model trained for a particular task (in the case of this HTR example, identifying a particular handwriting style) can be applied to similar content (other handwriting styles) with very good results. Transkribus has Public AI Models (transkribus.org) that have been created by users of the system and are then shared and can be reused by anyone.",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "ml-ai.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "AI & ML in Libraries Literacies",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe following quick little hands-on activities below were developed by the Digital Research Team for British Library staff as part of the Digital Scholarship Training Programme.\n\nActivity: Explore Natural Language Processing\nCopy and paste a paragraph of text from somewhere around the web, or from your own collections, and see how each of these cloud services handle it:\n\nCloud Natural Language\nIBM Watson Natural Language Understanding Text Analysis\ndisplaCy\nVoyant Tools (voyant-tools.org) Voyant Tools is an open-source, web-based application for performing text analysis. It supports scholarly reading and interpretation of texts or corpus, particularly by scholars in the digital humanities, but also by students and the general public. It can be used to analyse online texts or ones uploaded by users.\nAnnif - tool for automated subject indexing There are many video tutorials here and the ability to demo the tool\n\n\n\nActivity: Explore Conversation Generation (ChatGPT)\nLogin to use the freely available ChatGPT (openai.com) interface.\nTo get a useful response from ChatGPT, “prompting” is key. If you only ask a simple question, you may not be happy with the results and decide to dismiss the technology too quickly, but today’s purpose is to have a deeper play in order to develop our critical thinking and information evaluation skills, allowing us to make informed decisions about utilising tools like ChatGPT in our endeavours. Basics of Prompting | Prompt Engineering Guide (promptingguide.ai) gives a nice quick walk-through of how to start writing good prompts or you can take a free course !\nHave a play trying to get ChatGPT to generate responses to some of the questions here (or come up with your own questions!) Critically evaluate the responses you receive from ChatGPT, what are its strengths and weaknesses, ethical considerations and challenges of using AI tools such as this.\n\nIs the information/response credible?\nAre there any biases in the responses?\nDoes the information align with what you know from other sources?\n\n\n\nActivity: Explore Computer Vision & Handwritten Text Recognition\nFind an image from somewhere on the web, or from your own collection, and see how each of these cloud services handles it! Try with some images of basic objects to see results (cars, fruit, bicycles…) and images with text within them.\n\nGoogle Cloud Vision API\nVisual Geometry Group - University of Oxford\nTranskribus(Try for free, but does require a free account to login)",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#recommended-readingviewing",
    "href": "ml-ai.html#recommended-readingviewing",
    "title": "AI & ML in Libraries Literacies",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMuch of this topic guide is based on both a Library Carpentry Intro to AI for GLAM lesson I have developed with colleagues and a 2024 talk I gave as part of a Research Libraries UK Digital Shift Forum so if you’d like to view/listen to the content covered in this topic guide as a lecture:\n\n\n\nWatch the video\n\n\nThere are of course untold numbers of lists out there with resources for learning more about AI & Machine Learning but I think this particular guide is exceptionally useful in its coverage and topics selected, particularly as they are quite specifically for Librarians: Add’tl Reading for Librarians & Faculty - Using AI Tools in Your Research - Research Guides at Northwestern University",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "ml-ai.html#taking-the-next-step",
    "href": "ml-ai.html#taking-the-next-step",
    "title": "AI & ML in Libraries Literacies",
    "section": "Taking the next step",
    "text": "Taking the next step\nThe AI4Lam group is an excellent, engaged and welcoming international organisation dedicated to all things AI in Libraries, Archives and Museums. It’s free for anyone to join and is a great first step for anyone interested in learning more about this topic!",
    "crumbs": [
      "AI & ML in Libraries Literacies"
    ]
  },
  {
    "objectID": "collectionsasdata.html",
    "href": "collectionsasdata.html",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "Introduction: What is the concept of Collections as Data and how can my institution get started?\nGLAM (Galleries, Libraries, Archives and Museums) institutions have been making their content available to the public and researchers in a variety of formats for different purposes for decades. From maps, images, text, historical newspapers or postcards, digitising collections and displaying our items online are at the core of making this information accessible for research and enjoyment. But recent advances in and expansion of the use of AI and Machine Learning in library work and library research have transformed the way in which we and our users expect to access, view, use and search our collections. As the demand for computationally accessible cultural heritage collections continues to grow, and we’re being asked to provide our collections in the form of datasets for instance, how do we go about this practically?\nIn this new context, where GLAM institutions now find themselves playing a leading role as data providers and data curators, the Collections as Data concept emerged as a new approach, guidance and framework to support responsible development and computational use of digital cultural heritage collections. By computational use we mean the application of computational techniques such as natural language process, computer vision, and more to the analysis, exploration and reuse at scale of digitised cultural heritage content. For more practical use cases see our AI and Machine Learning in Libraries topic guide.\nThree particularly useful outputs from the Collections as Data community of practice are:\n“Want to support collections as data at your institution, but not sure how to begin? Drawing on what we learned from engaging with practitioners and researchers throughout the Always Already Computational project, the project team compiled a list of 50 Things you can do to get started. 50 Things is intended to open eyes, stimulate conversation, encourage stepping back, generate ideas, and surface new possibilities. If any of that gets traction, then perhaps you can make the case for investing in collections as data at your institution in a meaningful, if not systematic, way”\n“The Vancouver Statement suggests a set of principles for thinking through questions that collections-as-data work produces, as part of an expanding global, interprofessional, and interdisciplinary effort to empower memory, knowledge, and data stewards (e.g., practitioners and scholars) who aim to support responsible development and computational use of collections as data. This stewardship role only grows in importance as artificial intelligence applications, trained on vast amounts of data, including collections as data, impact our lives ever more pervasively.”\nDeveloped as a result of a community-led effort, the checklist covers different aspects such as providing a suggested citation, including documentation about the datasets (e.g., README files or tutorials) or sharing examples of use (e.g., prototypes or Jupyter Notebooks).",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#introduction-what-is-the-concept-of-collections-as-data-and-how-can-my-institution-get-started",
    "href": "collectionsasdata.html#introduction-what-is-the-concept-of-collections-as-data-and-how-can-my-institution-get-started",
    "title": "Collections as Data: Getting Started",
    "section": "",
    "text": "Fifty things (2018)\n\n\n\nVancouver Statement on Collections as data (2023)\n\n\n\nA Checklist to Publish Collections as Data in GLAM Institutions (2023)",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "collectionsasdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Collections as Data: Getting Started",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nMany institutions have started to adopt the collections as data principles to varying degrees, often combining publication of digital collections suitable for computational use, with a lab offering technical support to use them. You might notice that the examples below are all slightly different implementations of the principles though as each institution will necessarily need to decide for themselves, based on their individual goals and constraints (such as lack of resources, staff or IT skills) how to practically adopt the Collections as Data principles.\n\nData Foundry at the National Library of Scotland provides datasets in the form of downloadable files. Each of them includes a website with transparent information about the creation and curation of the content. It also provides examples of use and prototypes in the form of Jupyter Notebooks.\nNational Library of Luxembourg provides a digital collection based on Historical newspapers. It follows an innovative approach by providing different datasets in terms of the size and the content according to the purpose of the reuse (e.g., getting started or Big Data).\nBritish Library Labs provides a selection of datasets made available under open licences to experiment.\nDATA-KBR-BE is a project that aims at facilitating data-level access to KBR’s digitised and born-digital collections for digital humanities research.\nBiblioteca Virtual Miguel de Cervantes (BVMC Labs) is a digital library that published its bibliographic catalogue in the form of Linked Open Data. It also provides examples of use by means of of Jupyter Notebooks.\n\nMore advanced approaches are focused on the concept of data spaces, which are new cloud environments in which data can be shared for research use, while also allowing data suppliers to retain rights and control over the data. This can be useful for contexts in which the access of the data is restricted (e.g., geographic region, licensed content). A workflow to publish Collections as Data: the case of Cultural Heritage data spaces ) gives more information on this topic in the context of the European common data space for cultural heritage.\nIt is important to note that institutions need to carefully consider how and for what purpose they want to publish their digital content. For instance, the National Library of the Netherlands has recently restricted access to collections for training commercial AI. See our topic guide on Copyright and Licensing for more on that.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "collectionsasdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Collections as Data: Getting Started",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nOne simple activity you could do at your institution is to gather a group of like-minded colleagues together and have a read through Fifty things, Vancouver Statement on Collections as data and/or A Checklist to Publish Collections as Data in GLAM Institutions and have a look at some of the case studies above, and as a group consider the questions:\n\nAre there some activities suggested that are already underway?\nCan you identify one or two simple things that your institution could do now in order to start using these principles?\nWhat might you like “collections as data” to look like at your institution?\n\nIf you’d like to get a sense of how people are reusing collections published as data, the GLAM Workbench and the new computational access section of the International GLAM Labs Community website both have tutorials that can walk you through using cultural heritage datasets in a variety of ways.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#recommended-readingviewing",
    "href": "collectionsasdata.html#recommended-readingviewing",
    "title": "Collections as Data: Getting Started",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in reading more about the collections as data concept we highly recommend the Zotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets which is an open bibliography collaboratively maintained by the community of practice of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "collectionsasdata.html#taking-the-next-step",
    "href": "collectionsasdata.html#taking-the-next-step",
    "title": "Collections as Data: Getting Started",
    "section": "Taking the next step",
    "text": "Taking the next step\nJoining the International GLAM Labs Community is a great way to get started on the path of opening your collections up as data as well as the Collections as Data - Google Group as both communities are very active and have a wide range of experience and expertise to share on this topic.",
    "crumbs": [
      "Collections as Data: Getting Started"
    ]
  },
  {
    "objectID": "lod.html",
    "href": "lod.html",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "Introduction: What does publishing linked open data enable?\nThe Semantic Web was first introduced in the 2000s by Tim Berners Lee as an extension of the current Web. Instead of providing information in the form of documents and unstructured text like in traditional webpages, the Semantic Web facilitates the publication of machine-readable data on the web through standards such as Resource Description Framework (RDF) and Web Ontology Language (OWL).\nLinked Open Data (LOD) is a method of publishing structured data about things using the RDF to enable interlinking and semantic queries across datasets. The data is organised in “triples”, each consisting of a subject (e.g., Named Person), predicate (IsAuthorOf), and object (Book Title), identified by Uniform Resource Identifiers (URIs) to ensure global uniqueness and interoperability. It allows metadata to be connected and enriched, so that different representations of the same content can be found, and links made between related resources.\nHave a quick look at this video from Europeana explaining the high-level basic principle of LOD before we dive a bit deeper into how it practically works:\nLinked Open Data | Europeana PRO",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#introduction-what-does-publishing-linked-open-data-enable",
    "href": "lod.html#introduction-what-does-publishing-linked-open-data-enable",
    "title": "Linked Open Data in Library Use Today",
    "section": "",
    "text": "How it works\nRDF triples are the fundamental building blocks of Linked Open Data. Triples follow the RDF standard and consist of three components:\n\nSubject: This is the entity or resource being described. It is usually represented by a URI that uniquely identifies the resource.\nPredicate: This represents the relationship or property of the subject. It is also identified by a URI and specifies the type of relationship between the subject and the object.\nObject: This is the value or resource that is related to the subject. The object can be another URI (representing another resource) or a literal value (such as a string or number), amongst others.\n\nAn example of a triple stating “Miguel de Cervantes is author of El Quijote.” would look like this:\n\nSubject: &lt;https://www.wikidata.org/wiki/Q5682&gt; (Wikidata URI reference to Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core URI term for creator/author)\nObject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata URI reference to the book El Quijote)\n\nIn a 2020 survey of LIBER members, the LIBER Linked Open Data Working Group identified the following as the most frequently used datasets by libraries to enrich their catalogues but there are many more that can be used depending on your needs Linked Data Survey (oclc.org) and some examples of advanced data models are Bibliographic Framework (BIBFRAME) and Library Reference Model (LRM). In addition, the lod-cloud provides more than one thousand LOD repositories classified by categories and based on different domains such as geography and government.\n\nSo let’s go back to our triple that describes the relationship between the resource “Miguel de Cervantes” and the book of “El Quijote”. When different triples share the same URI for a subject, predicate, or object, they create a connection. For example:\nTriple 1: Miguel de Cervantes is author of El Quijote\n\nSubject: &lt;https://www.wikidata.org/wiki/Q5682&gt; (Wikidata identifier for the author Miguel de Cervantes)\nPredicate: &lt;http://purl.org/dc/terms/creator&gt; (Dublin Core term for creator/author)\nObject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata identifier for the work El Quijote)\n\nTriple 2: El Quijote is a work of Spanish Literature\n\nSubject: &lt;https://www.wikidata.org/wiki/Q480&gt; (Wikidata identifier for the work El Quijote)\nPredicate: &lt;http://purl.org/dc/terms/subject&gt; (Dublin Core term for subject)\nObject: http://dbpedia.org/resource/Spanish_literature\\ (DBpedia identifier for Spanish literature)\n\nHere, the object of the first triple (&lt;https://www.wikidata.org/wiki/Q480&gt;) is the subject of the second triple, linking information about the book to information about its subject matter. So an example catalogue record combining many triples then might look like:\n&lt;http://example.org/catalogue/El_Quijote&gt; &gt;rdf:type schema:Book; schema:name “El_Quijote”; schema:author &lt;https://www.wikidata.org/wiki/Q5682&gt;; schema:genre &lt;http://dbpedia.org/resource/Novel&gt;; schema:inLanguage &lt;http://id.loc.gov/vocabulary/iso639-1/es&gt;; schema:datePublished “1605”; schema:about &lt;http://dbpedia.org/resource/Spanish_literature&gt;; schema:about &lt;http://dbpedia.org/resource/Spanish_Golden_Age&gt;; schema:sameAs &lt;http://dbpedia.org/resource/Don_Quixote&gt;.\n&lt;https://www.wikidata.org/wiki/Q5682&gt; &gt;rdf:type schema:Person; schema:name “Miguel de Cervantes”; schema:birthPlace &lt;http://dbpedia.org/resource/Alcala_de_Henares&gt;; schema:birthDate “1547-09-29”.\n&lt;http://dbpedia.org/resource/Alcala_de_Henares**&gt; &gt;rdf:type schema:Place; schema:name “Alcalá de Henares”; geo:country &lt;http://sws.geonames.org/2510769/&gt;.\n&lt;http://sws.geonames.org/2510769/&gt; &gt;rdf:type schema:Country; schema:name “Spain” .",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "lod.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Linked Open Data in Library Use Today",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nGLAM institutions and in particular, libraries, have played a leading role in the publication of their data, primarily collections metadata, as LOD and using them including:\n\nBibliothèque nationale de France\nBiblioteca Virtual Miguel de Cervantes\nBritish Library\nEuropeana\nLibrary of Congress\nNational Library of Scotland\nNational Library of Spain\n\nAdditional examples from other related domains such as museums and Digital Humanities initiatives are the Rijksmuseum and Smithsonian American Art Museum, and Linked Open Data Infrastructure for Digital Humanities in Finland (LODI4DH).\nThe benefits of the publishing and use of the Semantic Web and LOD for:\n\nSemantic Enrichment: LOD helps libraries improve searchability and enables more precise queries by enriching existing catalogue records. Libraries have started to enrich their catalogues with external LOD repositories in order to provide additional contextual information that may be missing from your own catalogue (e.g., author nationalities (VIAF), geographic coordinates (GeoNames) relating to birth places of authors, or related subjects (Library of Congress Subject Headings). As in the example above a catalogue record for the book “El Quijote,” could be enriched with metadata about the author, language, publication date, related literary movements, and geographical information, all connected through LOD triples.\nInterconnectedness: LOD allows libraries to link their data with other rich datasets, creating a web of interconnected information. This enables users to discover related resources beyond their own library’s holdings. For example: a library could link their catalogue data with other LOD repositories, to enhance search results. Searching for “El Quijote” in the catalogue could return results not only from their own collection but also from other institutions that use LOD.\nIncreased Visibility: By publishing data as LOD, institutions can increase their visibility on the web as researchers, developers, and other institutions can easily find and reuse library data. For example: Adding information about a rare copy of El Quijote in your collection to Wikidata would aid its discovery through Wikipedia articles (Libraries and Wikidata: Using linked data to expand access to library collections worldwide – Wiki Education).\nInnovation: LOD encourages creative applications and tools. Developers can build new services, visualisations, and applications using linked library data. For example: LOD allows the creation of new types of visualisations, such as timelines, maps and graph charts that can be useful to gain insight, in some cases without the need to install additional software thanks to the use of APIs. Some examples include:\n\na tutorial in Spanish to create map visualisations based on Wikidata and using several data repositories (e.g., members of the International GLAM Labs Community) as content\nthe exploration of machine-readable visual configurations to browse LOD repositories provided by Cultural Heritage institutions, including libraries, in the form of Jupyter Notebooks\na map representing the geographic locations mentioned in the metadata provided by a corpus of historical documents and paintings.\n\n\nThough there are many benefits, SPARQL is the means by with Linked Open Data is queried and accessed and it’s worth being aware that the use of API’s based on SPARQL can be complex for less technical users since they need to understand how the data is modelled as well as be able to type a query. In addition, data quality has become crucial and several initiatives are focused on the assessment of the data quality provided by the catalogues.\n\nCase Study: Manuscripts on Wikidata: the state of the art? | by Martin L Poulter | Medium\nThis example shows how to use Wikidata, a community-driven approach based on the Semantic Web and LOD that enables volunteers to edit the metadata, to describe manuscripts. It shows the expressivity of the vocabulary provided by Wikidata and the benefits of using Wikidata as a repository in terms of visibility and reuse.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "lod.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Linked Open Data in Library Use Today",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nAs part of my National Research Librarian’s fellowship at National Library of Scotland exploring the adoption of Semantic Web technologies to transform, enrich and assess the Data Foundry’s digital collections, I created a collection of Jupyter Notebooks that enables users to:\n\nunderstand the benefits of the adoption of the Semantic Web;\ncreate an RDF repository from a traditional dataset;\nenrich a dataset with external repositories such as Wikidata;\nreproduce the analysis and visualisations based on the datasets created.\n\nI can also highly recommend starting with this Introduction to the Principles of Linked Open Data | Programming Historian tutorial which gives a great walk through of creating linked open data and includes an activity for using SPARQL to query LOD.\nThe course about Linked Open Data in cultural heritage collections, developed at Leiden University also includes a tutorial about a number of tools that can be used to create and to publish LOD. More specifically, it contains discussions of the LDwizard and CLARIAH Data Legend tool ‘COW’.\nTo be able to retrieve and analyse Linked Open Data, you need to know how to build SPARQL queries. The following course can be helpful:\n\nIntroduction to SPARQL\n\nExamples of SPARQL queries used to collect and analyse data from heritage institutions can be found in the notebooks below:\n\nThe Europeana SPARQL endpoint\nWikidata\nShort Title Catalogue of the Netherlands\nThe Dutch Institute for Art History",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#recommended-readingviewing",
    "href": "lod.html#recommended-readingviewing",
    "title": "Linked Open Data in Library Use Today",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nIf you are interested in learning more about LOD in terms of how to transform traditional bibliographic information into the Semantic Web, check out my research work performed as part of a fellowship at the National Library of Scotland in order to publish digital collections as LOD.\nI can also recommend the excellent Best Practices for Library Linked Open Data (LOD) guide published by the LIBER Linked Open Data (LOD) Working Group in 2021 which outlines in detail six steps for publishing library linked data.\n\nSome examples of research articles to read providing additional details and information include:\n\nTowards a semantic approach in GLAM Labs: The case of the Data Foundry at the National Library of Scotland\nLIBER’s Linked Open Data Working Group Publishes ‘Best Practices for Library Linked Open Data (LOD) Publication’ - LIBER Europe\nAn automatic data quality approach to assess semantic data from cultural heritage institutions\nA Shape Expression approach for assessing the quality of Linked Open Data in libraries\nAn Ontological Approach for Unlocking the Colonial Archive. (Example of transformation into RDF of a collection of maps using Open Refine.)\nEvaluating the quality of linked open data in digital libraries - Gustavo Candela, Pilar Escobar, Rafael C Carrasco, Manuel Marco-Such, 2022 (sagepub.com)\n\nYou can also find innovative ideas in the research articles published at the Semantic Web Journal.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "lod.html#taking-the-next-step",
    "href": "lod.html#taking-the-next-step",
    "title": "Linked Open Data in Library Use Today",
    "section": "Taking the next step",
    "text": "Taking the next step\nThe LD4 Community is a community of practice for linked data in libraries.\nLinked Art is a community working together to create a shared model based on LOD to describe cultural heritage with a particular focus on art.\nCode4Lib is a community effort including a mailing list and a journal providing open articles based on the library domain and including LOD.",
    "crumbs": [
      "Linked Open Data in Library Use Today"
    ]
  },
  {
    "objectID": "iiif.html",
    "href": "iiif.html",
    "title": "IIIF",
    "section": "",
    "text": "Introduction: What is IIIF?\nIIIF (pronounced “triple-eye-eff”) stands for the International Image Interoperability Framework. Quite a tongue twister that one, but it broadly represents two things:",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#introduction-what-is-iiif",
    "href": "iiif.html#introduction-what-is-iiif",
    "title": "IIIF",
    "section": "",
    "text": "a set of open standards for delivering high-quality, attributed digital objects online at scale.\nthe open, international community of software developers, libraries, researchers, educators, museums, universities, creative agencies, and more working together to develop and implement the IIIF APIs to make the above open standards happen.\n\n\nOpen Standard\nEver try to look at a large high-resolution digitised manuscript online only for it to take ages to load, and when it finally does you have no way to actually move around the image easily nor see any of the metadata or annotations related to it?\nOr maybe spent months on end negotiating the terms and methods around sending copies of a variety of differently sized individual images to another institution for them to host as part of collaborative project?\nIIIF brings a whole new efficiency to the way in which we in the cultural heritage sector go about sharing and making our digitised collections available online, while greatly expanding the functionality around the way users interact with them. It’s an open standard, collaboratively developed and maintained by a host of cultural heritage institutions around the world that defines a consistent method for the delivery of images and audio/visual files from servers to different environments on the Web where they can then be viewed and interacted with in many ways.\nIIIF basically specifies a way for browsers to display an image or audio/visual files in a way that enables much richer functionality on the Web:\n\nMakes it easier to display large images on the web in a way that is scalable (enabling deep zoom)\nAllows easy comparison between two objects, connecting and uniting materials across institutional boundaries\nDisplays structure and metadata and annotations with the digital collection item. (For a digitised manuscript for instance this might be page order and searchable text, for audio/visual materials, that means being able to deliver complex structures (such as several reels of film that make up a single movie) along with things like captions, transcriptions/translations, annotations, and more.)\n\n\n“At its simplest, IIIF uses APIs to load images quickly and zoom smoothly without additional loading time. But IIIF also allows you to do much more, including pulling IIIF-enabled images from different sites into viewers for comparison without downloading them (at full resolution), and enabling saving links to details of images or portions of A/V files for future reference. IIIF also allows you to use many open-source tools that help you to compare, annotate, transcribe, collaborate, and more. You can even gather multiple IIIF images together from across multiple archival collections and/or institutions to create projects or exhibits without advanced technical skills”. -From IIIF for Archives\n\nMaking your collections IIIF enabled makes it easier to share your content online in a consistent way, enabling portability across different IIIF enabled viewers. This means that rather than endlessly creating copies of your images all over the world for different projects, they can stay on your same IIIF Image server, but be accessed and displayed by viewers hosted at institutions elsewhere.\n\n\nOpen Community\nThe IIIF community is made up of over 100 major cultural heritage organisations worldwide who have formally adopted it, including many of our very own LIBER members. It was started in 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. It’s a really nice example of an open, grassroots but global community effort, backed by a consortium of leading cultural institutions, who have been working together to develop and implement this new capability for decades and solve their shared problems with delivering, managing, sharing, and working with their resources.\n\n\nWhat would I need to do to make my collection IIIF enabled?!\nIf you want the deep nitty gritty technical stuff around all the APIs and how they fit together there is quite a bit of implementation documentation on their website that goes into all this. But essentially, the basic set up behind making your own digitised collections “IIIF enabled”, as they say, looks a little something like this:\n\nSet-up a IIIF image server (you can choose one developed by the community, or there are IIIF-compatible image servers available from vendors or other web hosts), move your content there and implement the IIIF Image API to make those images and audio/video materials available from there.\nImplement the IIIF Presentation API which creates the all important IIIF Manifest files (also many open source or vendor products can help handle this bit too) for each of your objects. This Manifest file is really the prime unit in IIIF, it essentially combines and packages in json code, information about your images and structural data from your metadata source. It lists all the information that makes up your new IIIF enabled object, from how to display it to what information IIIF viewers should (and should not) display (such as structure or the order of pages, or even as minute as where an illustration is located within an image if you like). If you want to see an example of what one looks like this is a IIIF Manifest from the Bodleian Libraries at University of Oxford relating to this collection item. Each manifest has its own URL and that’s the bit you’ll use to do cool things with the object in different IIIF viewers, such as allowing a manuscript to be easily dragged and dropped into Mirador for instance for comparison with other IIIF enabled manuscripts.\nChoose one of the many IIIF enabled viewers for displaying your images and add it to your own collection site. Again, looking at that same collection item record above, note in the upper left hand-corner (see image below) there is an option to view in Mirador or Universal Viewer which are two different styles of IIIF viewer that afford different functionalities.\nConsider making your IIIF Manifests available publicly for download so users can work with them in all the interesting ways you’ve now enabled!",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "iiif.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "IIIF",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nIIIF was initiated by a bunch of libraries and cultural heritage collections holders and it shows! It was proposed in late 2011 as a collaboration between The British Library, Stanford University, the Bodleian Libraries (Oxford University), the Bibliothèque nationale de France, Nasjonalbiblioteket (National Library of Norway), Los Alamos National Laboratory Research Library, and Cornell University. The intention of the consortium has always been to combine resource and effort in building viewers that reflected the way we wanted our digital collections to be displayed online, rather than everyone still spending time and resource making our own custom viewers and building our own content silos. It’s brought a huge amount of efficiency too in the way that we share images with each other and researchers. It’s changed the way collaborative projects are undertaken where endless metadata mapping exercises, contract negotiations around re-use and hosting, and the copying and shipping of digitised materials on external hard drives were the norm.\nThere are quite a number of use cases and case studies available on the IIIF Demos page but let’s have a quick look at three real life (and canonical) examples of IIIF in action.\n\nDeep Zoom and Annotations\nThe example here is of the Ōmi Kuni-ezu 近江國絵圖 Japanese Tax map created in 1837. It’s meant to be read in the round by someone standing in the middle–you can see the scale when this zooms out– the map is eleven by seventeen FEET, and the person standing next to it, Wayne who works in the library at Stanford, is six feet four inches tall. This image is a composite of 158 individual images with a file size of 1.27Gb. IIIF allows just enough of an image to be delivered to a viewer–going from a whole image to just the part that they are zooming in on. Without IIIF, an end user might have to download an extremely large file, but using IIIF provides a smooth and easy viewing experience.\n\nHave a play and view this image in their Universal viewer here\n\n\nVirtual Reconstruction\nThe virtual reconstruction of this damaged manuscript from Châteauroux in France (Grandes Chroniques de France, ca. 1460) is probably one of the most well-known and best examples of the power of IIIF to support this use case (and my own personal favourite!). At some point in the manuscripts history, fourteen of its illuminations were cut out. These illuminations eventually ended up at the Bibliothèque nationale de France in the 19th century and were digitised individually. In the demo you see the reuniting of the miniatures with the full manuscript as IIIF allows a virtual repositioning of the cut out decorations with the text, virtually reconstructing the manuscript online using the Mirador Viewer so it reflects its original state.\n\nI highly recommend having a play around with the Mirador Viewer: Châteauroux demo.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "iiif.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "IIIF",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe following tutorials are two of my favourite recommendations for colleagues interested in having a play with IIIF manifests yourself and in the process gaining a practical understanding of how they work, and the benefits!\n\nIIIF Online Workshops The community itself has created a number of excellent and free self-paced tutorials and though they host live online workshops for a fee, these are recorded and available and useful for newcomers for free afterwards. In fact staff at British Library have walked through these self-guided resources online quite often with great success. There is also the opportunity to hire a IIIF Trainer to come to deliver live bespoke training directly to your institution (for a fee), which we have also partaken in!\nWorking with IIIF images in education, communication and research This is a self-guided workshop available online in Dutch and English and has some excellent exercises to get you familiar with finding IIIF manifests in catalogues and importing them into different viewers. I highly recommend making some time (they recommend 120 minutes) to read through this and try out some of the exercises.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#recommended-readingviewing",
    "href": "iiif.html#recommended-readingviewing",
    "title": "IIIF",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nThere is a useful collection of articles and data related to IIIF being compiled by the community at https://zenodo.org/communities/iiif/records?q=&l=list&p=1&s=10&sort=newest\nThe IIIF organisation has also created a number of useful resources alongside their training materials such as How It Works, a plain-language guide to how the IIIF API’s work and a glossary of “Key concepts you’ll encounter when working with IIIF”",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "iiif.html#taking-the-next-step",
    "href": "iiif.html#taking-the-next-step",
    "title": "IIIF",
    "section": "Taking the next step",
    "text": "Taking the next step\nLearning about IIIF can be overwhelming at first, especially if you’re not a programmer, but the IIIF Community is a very supportive and engaged one and has created a number of ways to get involved and find support and help.\nI recommend checking out their community page IIIF Community to find details of their next open community calls, or to join their Slack Channel where you can post questions and join the discussion with other users.\nThere is also a massive list of resources, Awesome IIIF, compiled and maintained by the IIIF Community if you are looking to take your knowledge a bit further and dig deeper into some of the exciting implementations of IIIF.",
    "crumbs": [
      "IIIF"
    ]
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualisation",
    "section": "",
    "text": "Introduction\nWhy do we visualise data? For data to tell us something we need to look for patterns, and we are much better at finding these patterns in colours and shapes than in a table of raw data. Visualisations are key to developing the story we want to tell with our data.\nWhen do we visualise data? There are two main moments when working with data that we need to visualise it. First, in the exploratory phase, when we are trying to understand the data, to get insights that lead to fruitful lines of enquiry. These visualisations are rough and ready, meant for us or at most a select few around us to generate debate about what information the data might contain.\nSecond, in the explanatory phase, when we have understood the data and move on to insightful analyses that generate new understanding. These visualisations communicate what we have learned from the data to others. They need to be clear, because we are trying to explain conclusions we have made from intimate knowledge of the data to people who have not worked with it, and are trusting us to explain faithfully what we have learned.\nIn libraries this covers visualisations for users of our services designed to improve content discovery and so they can understand our data, as well as research carried out by librarians and digital humanists that generates new knowledge. Particularly for our users, whose needs should be at the heart of the services we supply, visual routes we supply into our data need to be as clear as anything we’re presenting as research.\nHow do we visualise data? There are a huge variety of tools available. Practitioners may lovingly hand draw images (physically or digitally), from early 19th century visualisations by Florence Nightingale and W. E. B. Du Bois to the work, verging on art, of Federica Fragapane. More commonly there are deeply customisable packages in modern programming languages like R, Python or Javascript, or commercial plotting software like Tableau. Excel has endured through its simple learning curve, ubiquity and reliable outputs. A whole separate suite of software exists for geospatial and linked data. Ultimately the right choice is decided by our use case, data, resources and skills.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "dataviz.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Data Visualisation",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nExploratory data visualisations are both quite generic and very dataset specific. Simple plots like bar charts, histograms, scatter plots, and time series are simple enough to be reliable and fit the ethos of being quick and informative of the features of your data. This Power BI dashboard from Brandi Jagars at University of South Florida Libraries shows how simple visualisations give quick insights into a visitor dataset.\n\n\n\nA dashboard of exploratory graphs\n\n\nVisualisations designed for users and for publication are much more varied, and (usually) more polished. Maps are an eye-catching and engaging interface into collections that tap into users’ sense of place. The Mapping Manuscript Migrations project offers a map view of global manuscript migrations allowing users to track the movement of manuscripts filtered by things like collection, author and date of publication.\n\n\n\nMovement of manuscripts from the collection of Sir Thomas Phillipps\n\n\nPeripleo is a browser based map viewer that can be used with any cultural heritage dataset with associated location information. It was used in the Heritage for All project to displaying items in hyper-local contexts.\n\n\n\nMap of cultural heritage item locations around Exeter\n\n\nBoth of these initiatives rely on the concept of Linked Open Data. Each element of linked data is linked to other elements by one of a defined set of relationships, rather than the traditional spreadsheet model where each row is an item with a certain set of properties. This transforms the data into a network, which allows for intuitive, interactive visualisations that let users navigate material contextualised by the items closely linked to it.\nThe Semantic Name Authority Cymru (SNARC) provides views into a linked database of name authority records linked to Wales and the Welsh language. This makes graphs like this family tree of Charlotte Guest, a translator, businesswoman and noble, easy to produce. Displaying parts of the network let users understand the connections within it, as in this rather large, but very satisfying, graph of Welsh estates, houses and their owners.\n\n\n\nSNARC Welsh estates network graph\n\n\nThere’s also huge value in making visualisations like this available in physical form within the spaces of a library. The Bohemian Bookshelf was a 2011 project to increase serendipitous book discovery that was installed in the University of Calgary Library. It used 5 different visualisations to ‘entice curiosity’ by taking users to books and authors they might not have otherwise explored. This echoes Dr Mia Ridge’s proposed metric of ‘clicks to curiosity inspired’ in seeking ways to make it easy for users to be inspired by the collection. The ‘book pile’ arranged books by size and page count, acknowledging our natural fascination with the very large and the very small.\n\n\n\nThe Bohemian Bookshelf ‘Book Pile’\n\n\nYou can explore other use cases in this helpful list from the University of Minnesota Libraries. They’ve catalogued library specific resources for a range of use cases like the teaching, evaluation, and history of data visualisation in libraries.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "dataviz.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Data Visualisation",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nThe best way to understand the value of data visualisations is to produce them for your datasets. Here are a few tools you can plug datasets into, organised by type and the skills needed to use them.\n\nImmediate results\nRAWGraphs is an online platform (no sign up required) you can upload a spreadsheet to (save it as a CSV first) or a JSON file and point and click your way down the page to produce a visualisation. It’s perfect for exploratory analysis and learning about the different kinds of visualisations available.\nVoyant Tools provides a similarly easy entry for corpus scale text data (whole works, or collections of works), though it helps if you know a little about corpus linguistics. There are ‘pre-built’ corpora of Shakespeare, Austen and Frankenstein available if you don’t have your own files to upload.\n\n\nSpreadsheet based\nExcel remains such an easy way to interact with spreadsheet data. If you haven’t used it before there are lots of resources available online or your institution may have Microsoft Office skills courses. This gentle intro from University of Minnesota libraries assumes some knowledge but not too much. PowerBI is a more advanced Microsoft Office app that allows you to create dashboards from data. It interfaces easily with Office software but the visualisations aren’t hugely inspiring. Justin Kelly has an introduction for librarians. Google Charts and Sheets and OpenOffice Calc and Impress are equivalent alternatives.\nTableau is one of many commercial softwares for visualisation. There’s a learning curve similar to Excel, and a free (sign-up required) public platform you can try in browser, use the learning resources to get started.\n\n\nIntro to coding\nThe R for Data Science Data Visualisation tutorial covers using ggplot2, the de facto standard for plotting in R. You can code along with an R environment in browser using Posit Cloud (requires a free account).\nSeaborn is one of the main plotting packages in Python and follows a similar philosophy to ggplot2. You can code along to their tutorial with a Python environment in browser using Google Colab (requires a Google account).\n\n\nMapping and Linked Data\nGeospatial and linked data (where elements of the data can be explicitly linked to other elements) have their own worlds of tools, with functionality also often covered by the types of tools already listed. ArcGIS (now common in its online form) and QGIS are the most common paid and open source Geographic Information Software (GIS) tools available. You can use these to work with data with geographic components and make display worthy maps. Their outputs also plug into Python, R and JavaScript libraries like leaflet or Dash. Programming Historian have a series of mapping lessons, from an intro to QGIS to converting historical place names into locations on a map.\nFor linked data tools like Gephi or Nodegoat have graphical user interfaces, or there are programming packages like igraph, NetworkX, and D3.js.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#recommended-readingviewing",
    "href": "dataviz.html#recommended-readingviewing",
    "title": "Data Visualisation",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\nMuch data visualisation is communication, and so is deeply subjective. Good visualisations are guided by design philosophies built upon how we process visual information. Two influential books that develop these philosophies are The Grammar of Graphics by Leland Wilkinson and The Visual Display of Quantitative Information (2nd Ed.) by Edward R. Tufte. The Grammar of Graphics is a hefty tome that proposes a core set of components for graphics then builds them from the ground up, with reference to programming. Visual Display is perhaps a little more accessible and uses the idea of how ink is used (digitally or physically) to understand what is and is not important in a graphic.\nIf visualisation is communication and how we understand visual communication is subjective, then catering generously to how different people process visual information is important. We might call this accessibility. This talk explains some of the basics of how our brains handle colour, and the importance of colour in visualisation. The Seaborn explanation of colour palettes is a helpful reference, and there is a free colour blindness tester to check that your visualisations encode information in colours that everyone can distinguish. Use alt-text for screen reader access and review for keyboard and content accessibility. Harvard have a helpful guide.\n\n\n\nThe importance of colour in recognising categories\n\n\nWith these considerations in mind there are graphics catalogues that help guide you towards the best visualisation for your purposes. from Data to Viz starts with types of data and leads you to appropriate graphs, while the Financial Times Visual Vocabulary starts with relationships between elements of your data and guides from there. Both are valuable.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "dataviz.html#taking-the-next-step",
    "href": "dataviz.html#taking-the-next-step",
    "title": "Data Visualisation",
    "section": "Taking the next step",
    "text": "Taking the next step\nData visualisation, like any other skill, takes practice and familiarity with tools to get the best results. It is, however, relatively easy to make simple but effective visualisations that help you understand your data or explain it to someone else. So take some of your data and begin to play around with it. Play is an important word, you are being creative! Make mistakes and do the unexpected as you learn and you will be better for it. It is, unfortunately, just as easy to make bad visualisations that confuse you and your audience. Start simply, more complex visualisations will come with time.\nIf you haven’t done any visualisation before start with something like RAWGraphs or Excel for tabular data, or Voyant Tools for text data. If you have geographic data find a GIS specialist, or use the tutorials for QGIS or ArcGIS. If you have some experience with visualisations then tailor your choice of tool to your data. And if you’re familiar with programming then look at the plotting packages available, and enjoy the flexibility and reproducibility they give you.\nEngage with designers if they’re available to you and you are producing work for users or the wider public. The art of designing things for people applies as much to data visualisation as it does to anything else.\nAbove all remember your audience. Keep clear in your mind what it is you are trying to communicate, and to who, and ask yourself if your visualisation does that. Continue to iterate until it does, and you will have explained the story in your data clearly.",
    "crumbs": [
      "Data Visualisation"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "",
    "text": "Introduction\nGitHub is a collaborative software development platform. Like a kind of google docs for coders, it began as a tool for them to collaboratively work on software development projects, ie, allowing different coders to contribute to parts of an overall project, asynchronously and across teams and even international borders, keeping track of changes all the while to make sure the end result is one cohesive software.\nGitHub today however is used for many more different purposes by people in all different fields, not just software developers and software projects, to:\nGitHub allows people to collaboratively work on and share projects (called ‘repositories’) so that others can copy them, either to adapt or to contribute back to the original project. That’s because any ‘public’ repository of files on GitHub is accessible via the web. (Note that public repositories are created by default when you start a project, but it is also possible to make your repositories private if you like in the settings).\nRepositories may be filled with code, but they may also be filled with pages of text written in markdown and this markdown can be rendered by browsers as webpages. This very website you’re reading from right now is being hosted on GitHub!\nThis guide will help you to understand how to practically navigate and contribute to projects like this one hosted via GitHub, and will demystify some of the basic actions and jargon around using GitHub along the way.",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#introduction",
    "href": "github.html#introduction",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "",
    "text": "record and share information on a collaborative project\nwork asynchronously and internationally\ntest and store scripts and technical documentation\nbuild a public website for a project\n\n\n\n\n\nGit, GitHub and GitHub Pages\nBefore we begin, it can be helpful to first grasp what Git, GitHub and GitHub Pages each are and how they relate to each other:\n\nGit is a distributed open source version control system. Version control allows you to track changes in a set of files. It does this by taking snapshots of repositories at each stage of development - in Git these snapshots are known as ‘commits’. In software development this means things can be tested and rolled back, and enables sharing of stages of development. Git can be used on its own but when used with GitHub has more potential for collaborative projects. You can download and install Git on your own machine. It runs in the command line or there are desktop GUIs available. Git for Humans (Alice Bartlett talk at UX Brighton 2016) is a nicely accessible introduction to the purpose and uses of Git.\nGitHub is a web-based software development platform. It has many of Git’s features and can host Git repositories but also provides a web interface and additional functionalities. You can view millions of repositories hosted there. Many are openly licensed, meaning you can freely copy (or fork) them, either to adapt for yourself or to contribute back to the original project. By default the files and folders in a repository you create are public, and with a little bit of extra code, a repository can also be turned into a more public-facing project website, blog or wiki using a feature called GitHub Pages.\nGitHub Pages is a feature within GitHub to turn any repository or project into a website. GitHub can also be used to host a site that has been created using other software packages. One example of such a software is RStudio, a visual editor for Quarto Markdown which is what we use for this DS Essentials project!\n\n\n\nThe language of GitHub\nA barrier to GitHub for beginners is that it has its own terminology for common tasks and actions. While off-putting at first, they do serve to outline core concepts of both Git and GitHub. When using Git in particular, these are important to understand as they relate to git commands such as fork, pull, commit, clone, or branch. There are many quick guides and cheat sheets to this terminology on the web which can be useful to use as a reference while familiarising yourself:\n\nGitHub Docs Glossary\nGit for Librarians Glossary\n\nIn the next section, we will demonstrate the actions behind these keywords as we walk through three types of ways librarians regularly interact with GitHub based projects:\n\nWriting an Issue\nContributing code/content to an existing project\nRe-using existing code/content for a new project",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "github.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\nBoth Git and GitHub have many potential applications for librarians and libraries. Library systems, digital collections and digital preservation are key areas where many resources and scripts are community-led and open source, made available via GitHub and other code repositories. For those working with open source platforms such as Omeka, a basic understanding of GitHub provides access to a host of plugins developed by user communities that can be cloned and applied to your local instance where you may not have resources to develop them yourself. In less technical fields of library work there also is the opportunity to use it to develop or share documentation of working groups and special projects. In all of these, there is the facility to raise an issue or fork a repository and make a pull request if you are looking to query or contribute to a project.\nHere are some examples of library activity on GitHub from Week one reading for “Git and GitHub for Librarians” course and Library Carpentry Library Carpentry: Introduction to Git: Summary and Setup, with some personal additions of our own:\n\nSharing documentation and code for library-related projects and platforms:\n\nLiving with Machines\nBritish Library Repositories\nOmeka exhibit platform and related plugins\nDigital projects of Library of Congress\nCollection Builder\n\nDistributing OCR’d text extracted from digitised materials for collections-as-data analysis [example] [example] [example]\nStoring scripts for executing metadata ingests and transformations [example, example, [example]\nArchiving the source texts of open educational resources [example]\nWebsites for library workshops, to share course material or sample datasets [example, example, example],\nCollaborating on and contributing to a project [Digital Scholarship & Data Science Essentials for Library Professionals](https://libereurope.github.io/ds-essentials/)\n\nLet’s look more closely at some practical ways you can get started navigating GitHub and contributing and making use of GitHub based projects for your library work!\n\n1. Writing Issues\nIssues are a great tool in GitHub to let the people know that something is possibly broken or something new is needed in a project. They help everyone by providing a place for everyone to discuss the concerns raised by the issue and providing a way to manage a response. If you have ever raised a ticket with the helpdesk of your IT department, you will notice a lot of similarities with Issues.\n\n\nPreparing to raise an issue\nIssues are a human centred tool in GitHub, so it is up to everybody involved to get the best out of them. Before raising an issue, consider these steps:\n\nRead the documentation carefully to make sure what you are experiencing is not down to any misunderstandings about the functionality or content provided.\nHave a look through the previous issues to make sure that what you are experiencing has not been raised before. If it has then consider adding details to that ticket and clicking on “Subscribe” under Notifications to be kept up to date with any fixes or changes.\nYou can raise issues for a variety of reasons, not just faults. You can also ask for new functionality or make a suggestion. Be clear, if you can, on the type of request you are making as the project maintainers may have different processes for different request types.\nRemember that what is obvious to you might not be obvious to a project maintainer, so explain fully and be patient with those trying to address the issue.\n\n\n\nReporting Bugs\nRaising an issue is a great opportunity to help to get something fixed. Providing good quality and complete information can cut down the time it takes to resolve an issue. A good write up should contain these elements:\n\nA full description of what happened, including:\n\nAny error messages or codes\nWhere you were in the project (e.g. for a web application what page you were on)\nWhat you were trying to do\n\nWhat did you expect to happen? Including:\n\nWhat normally happens\nWhat should have happened\nIf the behaviour is new\n\nAny other relevant information: the project maintainer will be looking into possible causes of the issue you are experiencing, so extra information can help enormously. For example of a web application this might include:\n\nThe time of day the error occurred\nWhich browser you use\nIf you have experienced any similar errors with other sites\n\n\n\n\nRequesting new features\nIssues are not only for reporting bugs. They can be used for requesting new features or further support. In this case explain why it would make sense to add your new feature to the project. Think about who might be affected by your change, both in a positive and negative way. Lastly, consider having a go at the change yourself and submitting a pull request. If this is not possible explain why here.\n\n\nThe life of an issue\nOnce you submit an issue you will see that it will have the status of “Open”. Usually, the issue will then be assigned to a specific member of the project team to examine further. During this process you may see comments added discussing the issue and possible implementations, fixes or feedback. The issue may even have some labels or categories applied to it to help classify the issue type. You may be asked for more information or input to help resolve the issue.\nWhen work on the issue stops, it will be “closed”. In GitHub there are two types of issue close:\n\nClose as completed: This means the issue has been worked on and that work is complete. E.g. this might mean a bug has been fixed or a feature added.\nClose as not planned: This means that the project maintainer has chosen not to work on the issue. This can be for a variety of reasons such as:\n\nWon’t fix: Adding a fix for this feature is outside the current scope of the project, beyond its resources or deemed unnecessary.\nCan’t reproduce: In software, developers must usually reproduce the error in order to fix it. Sometimes this is not possible to do, this might happen for intermittent errors or errors caused by particular hardware configurations.\nDuplicate: The issue has been reported elsewhere. There will usually be a reference to where this is.\nStale: Sometimes issues stay in the system for a long time. This might be because the initial reporter has moved on and is unable to answer queries about it. In this case the issue will be removed.\n\n\n\nWith an issue, there is no expectation that you will provide a solution to the problem you are facing or the new functionality that you would like. At some point you might contribute a fix or something new to a project, maybe as a result of an Issue. When this happens you will raise a “pull request”. This is a proposal to make a group of changes to files in a git repository. A maintainer can then decide whether to accept the pull request and “merge” it with the project, reject it, or send it back for further work.\n\n\n\n2. Contributing content/code to an existing project\nAs you now know, GitHub hosts many open-licensed projects and by clicking the fork button, any GitHub user can instantaneously create their own fully independent copy of that project to work on without affecting the original. This copy or forked project can then be used to work out new features in a piece of software (or in our case, writing new content for a project website) that can eventually be merged back into the original project. Let’s walk through a simplified example using the example of contributing substantial edits to one of the pages of DS Essentials. Let’s say you’d like to write up a new use case on one of the Topic Guides. That process may look a little like this:\n\n1. Find the GitHub repository/source code behind the website\nOn the web version of DS Essentials, look for the GitHub logo in order to be taken to the GitHub repository behind it.\n\n\n\n2. Make your own copy of the project (repository) to work on\nOnce on the project GitHub page you can fork your own copy of the repository so that you have a full copy of the project in your own GitHub account. You will now be owner of this new version and can make edits and changes as you please to the files without it affecting the main project. Just remember that eventually if you want your contributions to be merged with the original project those changes will need to be reviewed and approved by that maintainer.\n\n\n\n3. Find the file you would like to change and make your edits\nNavigate to the file you would like to edit in your own copy of the DSEssentials project and click the pencil icon to edit. (Note that one way you’ll know you’re working in your own copy is by seeing your personal GitHub username ahead of the repository name). Let’s say the Topic Guide on AI & ML in Libraries Literacies needs an update. To find the right file in the directory you can do a search or look for a markdown file in the list which corresponds to the url on the website version and has the file extension .md. Note, because this website was built using Quarto, the file extension in this example is .qmd. For instance if ml-ai.html is the web version, you would look for ml-ai.qmd in the file directory on GitHub.\n\nClick on the little pencil icon to edit this file in markdown. Markdown is a simple system for marking sections of text that should be stylised in a certain way, e.g. made bold or appear in a list of bullet points. Markdown files have the file extension “md” and can be edited in GitHub or with a text editor like Notepad. One common file in GitHub repositories that you will find in the markdown format is the “README.md” file which usually covers the purpose of the repository and how to get up and running.\nMarkdown works by using certain text characters to indicate styles. For example, placing a “#” in front of a line will make it a first level header. Placing “##” before the line makes it a second level header. Putting an asterisk around a piece of text, like this “*my text”, will cause that text to be italicised as my text. You can see more markdown commands and experiment with it on the site Markdown Live Preview.\n\n\n4. Commit your changes\nWhenever you are working in a repository that you have forked, you are the Owner of this copy of the original project and can commit (save) any changes and edits you make to files directly to this copy of the project without needing any approval. Committing builds up a history of changes that you can roll back as needed along the way. Each time you commit you have an option of writing a comment as to what has changed which is a useful tool to use and habit to get into for going back in time if you need to reverse something.\n\n\n\n5. Contribute your changes back to the original/main project\nWhen you are finished with your changes and would like to share your new updated copy with the original project (in this example, incorporating your new text on Generative AI into the main DS Essentials Topic Guide on AI & ML in Libraries) this is when you will start a pull request. Navigate to the Pull Requests section of your forked project (repository) and click on New pull request to start that process.\n\nThis will take you to back to the original project you forked from, where you can then formally “create a pull request”. Once this has been made your pull request notifies the maintainers of the original project that you have made changes to their project that they may like to consider merging into theirs. Your pull request will show up under their project where you can discuss and review the changes you’re suggesting. Once the review has been completed and everyone is happy with the changes suggested the maintainer of the project will merge your pull request and you will have officially contributed!\n\n\n\n\n3. Re-using existing code/content for a new project\nLet’s say that instead of just contributing code/content to an existing project you want to make use of the whole project, such as a piece of open source software shared there. This is quite common for instance when looking to reuse software that someone has created and shared on GitHub. And it’s a similar process to the above although in this case you might want to clone the repository rather than fork it as cloning allows you to save the whole repository directly to your local machine, rather than within the GitHub platform itself in order to implement it. Developers will find this much more practical than working in GitHub as they go through the process of installing, editing, adapting the code as needed to implement it. See for instance the example mentioned earlier of a variety of open source Omeka plugins being available for install from the Omeka GitHub repository.",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "github.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nLike many technologies, the best way to learn git or GitHub is to use it. Luckily there are lots of tutorials out there with ready-made exercises and walkthroughs if you don’t have a clear purpose or aim to start with. Depending on your learning style, there are written lesson plans like the Carpentries’ ones, or plenty of walkthrough videos covering everything from basic terminology to full workflow development. Many of these resources begin with a grounding in git, the locally-installed version control software. While not essential to using GitHub, this can be useful to gain a broader understanding of the workings of both git and GitHub and maybe make sense of some of their additional functionalities. There are also specific lessons on various topics such as developing a static website using just GitHub and GitHub pages.\nOur list below is not intended to be comprehensive but is a selection of what is out there and all have been used by library staff like ourselves to get to grips with Git!:\n\nLearn the basics\n\nThis official GitHub tutorial teaches you GitHub essentials like repositories, branches, commits, and pull requests.\nAnother great tutorial over on GitHub uses the Spoon-Knife project, a test repository that’s hosted on GitHub.com that lets you test the fork and pull request workflow.\nGit-for-librarians exercise on branching and one on forking\n\n\n\nMaking a website using GitHub\n\nMaking a website with GitHub & GitHub Pages (lesson plan with video)\nMaking a website with GitHub & Quarto / RStudio (video)\nProgramming Historian - Building a static website with Jekyll and GitHub Pages\nProgramming Historian - Running a collaborative research website and blog with GitHub Pages\n\n\n\nWriting in Markdown (tools to try)\n\nMarkdown Live Preview is a tiny web tool to preview Markdown formatted text.(https://markdownlivepreview.com/)\nWord to MD is a useful tool for uploading word files and transforming it into markdown\n\nAs with most platforms, specific functions or tasks may change occasionally on GitHub. So you may find a lesson which refers to an old term or function that is no longer called what it used to be. GitHub documentation should provide up-to-date information on the current practice in most cases.",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#recommended-readingviewing",
    "href": "github.html#recommended-readingviewing",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nLibrary Carpentry Git & Version Control / Software Carpentry\nPush, Pull, Fork: GitHub for Academics (hybridpedagogy.org)\nA Comparative Analysis of the Use of GitHub by Librarians and Non-Librarians\nA reading list for librarians learning about Git and GitHub\nWeek one reading for “Git and GitHub for Librarians” course.",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "github.html#taking-the-next-step",
    "href": "github.html#taking-the-next-step",
    "title": "GitHub: How to navigate and contribute to Git-based projects",
    "section": "Taking the next step",
    "text": "Taking the next step\n[TODO needs content on networks for help with GitHub]",
    "crumbs": [
      "GitHub: How to navigate and contribute to Git-based projects"
    ]
  },
  {
    "objectID": "workingwdata.html",
    "href": "workingwdata.html",
    "title": "Working with Data",
    "section": "",
    "text": "Introduction\nData is all around us, and it almost cannot be avoided. Libraries and universities are full of data, but staff often think they do not work with data. Have you ever been asked how many loans a book has had? What about how many visitors came in at the weekend? Or if a certain book is available? These are all data questions, answerable with quantifiable facts.\nData is often considered to be a number, like the number of times a book has been loaned, but what about who borrowed it, or what their review was? These are all different forms of data, and can provide all new insights to the popularity of a book - maybe a book was borrowed 10 times, but in fact got 10 1-star reviews, versus a book borrowed 5 times with 5 5-star reviews, using this data, which one would you recommend?",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#introduction",
    "href": "workingwdata.html#introduction",
    "title": "Working with Data",
    "section": "",
    "text": "What is Data?\nData is often overwhelming (the term ‘Big Data’ floats around a lot!), but that shouldn’t stop you from exploring it, because it can greatly improve your work. Just to clarify, Big Data will not be discussed here, Big Data requires data to be high in volume and velocity, which is not the case for most library data (think about downloading a document every time the hashtag #photooftheday is used).\nSo first, let’s establish some definitions (for this guide at least) while looking at one of our most central collections of data:\nA new book has arrived, you create a record on the library management system filling in the title, ISBN and price.:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nData\nA collection of information.\n\n\nData Point\nSingle point of data (e.g. a data point could be the title of a book, however alone that information tells us very little about the book. Therefore a ‘record’ containing relevant data points will be created.).\n\n\nMetadata\nData that provides information about other data (e.g. columns of data in a spreadsheet). (e.g. in a library catalalogue the metadata about the book includes the price information, the isbn, author and title\n\n\nDataset\nA collection of data from a single source or for a single project. (e.g. we might create a dataset of all catalogue records relating to a particular topic of interest to researchers)\n\n\nDatabase\nA table with structured (organised) data.\n\n\nDataframe\nA structured representation of data\n\n\nSoftware\nProgram for a computer.\n\n\nHardware\nThe physical components of technology.\n\n\nBack-end\nThe part of a system that is not usually visible or accessible to a user of that system.\n\n\nFront-end\nA software interface designed to enable user-friendly interaction with software.\n\n\n\n\n\nWhere is the data in a library?\nData can be qualitative (sentiment) or quantitative (numerical), therefore when we think about visitors to a library, do you know how they felt about their visit (qualitative data) or how many hours they spent at the library (quantitative data)?\nHere are some examples to get you thinking about what data points stem from a library;\n\n\n\n\n\n\n\n\nInstitution\nStaffing\nAcquisitions\n\n\n\n\nThe Building / SpaceWider CommunityStakeholdersFacilitiesBusinessOpening Hours\nLibrary StaffInstitution DepartmentsQualifications and Education\nSubscriptions (ongoing)Purchasing (one time)SoftwareSuppliersSpending / Financial Information\n\n\n\n\n\n\n\n\n\n\n\nResources\nEngagement\nPatrons\n\n\n\n\nMaterials metadataCollection(s)Care and MaintenanceDiscovery ServicesAccessibility Services\nResearch Data ManagementTrainingOutreachToursCommunicationQuantity of Users\nHelpdesk (in-person)Queries (email, online chat etc.)User SupportFeedbackInteractions\n\n\n\nConsidering the above, what do you interact with? Do you consider yourself to work with data?",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "workingwdata.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "Working with Data",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\n\nData exists, so what?\nData should have a purpose. If data has no purpose, why waste your time managing it? The purpose of data could be any of the following;\n\nProvide insights to pertinent questions, and thus make informed decisions.\nProvide evidence of a decision or conclusion.\nExplore relationships / trends, to tell a story and provide insights.\nExplain (and display) complex information more effectively.\n\nData can provide additional power when making a decision (big or small) - knowing exactly how many users visit at the weekend can prove the library does require the funding and staffing to stay open on the weekend. However, if it proves no one is visiting, do not fudge the numbers. Use those numbers to question why, such as what else could attract users, what do patrons use it for currently, or was there a cause to the decline in weekend patrons?\n\n\nProof is in the Pudding\nKnowing that data exists in a library is the first step, but what can be done with it? While this guide has touched on a few questions you can use data to answer, below will explore real world examples of libraries utilising their data to make powerful changes.\nData can inform decisions about:\n\nCollection Management: what is being used, when, by who, and why (if you have a review system).\n\nExample: Within our collection, how often do users borrow material published before 2000? (But remember, borrowing is not the only form of usage!).\n\nOutreach: who forms our community, what matters to the community,\n\nExample: Does the demographic of people attending our events reflect the demographic of people in our wider community? Which events could make the library more appealing to the wider community?\n\nFunding / Expenditure: Use data about patron usage, reader requests or patron feedback as part of an application for additional funding or staffing to prove your standing within the community.\n\nExample: How much would it cost to buy an additional copy of a book, if it had more than three reservations at a given time?\n\nReviewing Success: When starting a new project or approach, consider how it will be measured as a success. Knowing how to measure the impact of a project will enable you to actively reflect and produce solid facts about the success of your project.\n\nExample: How can we measure patron satisfaction currently, so that at the end of the project a 10% increase can be quantified and thus the project can be considered a success?\n\n\nHere are a few real world examples of big data projects within libraries, but remember, small or internal projects should be informed by data too.\n\nTelling Stories with Library Data July 2021\n\nThe Metropolitan Museum of Art (New York) Watson Library explores their production of data visualisations using Microsoft Power BI in regards to their library activities. Using the library management system (data which they already had), google analytics, digital collections and some manual tallies, they explored six years of library data. This exploration demonstrated the benefit of moving their blog from an external website, created an index of African American Artists,and ease in sharing library metrics for the previous six years publicly.\n\nWhen is a Year Complete? October 2023\n\nIt is well known that publications databases take time to update, but how long? Collection Analysis Librarian at Iowa State University, Eric Schares, wanted to know how long to wait before being able to analyse a calendar year of publications, and set about doing so by comparing data from three different publication services (Dimensions, Web of Science and Open Alex). The data (which continues to be updated), documented the rise of Open Alex, and demonstrated part of the effectiveness of each service for Iowa State University - however he also shares his code and thus you can replicate this data question at your own institution.\n\nUse of Institution Data Analysis for Publisher Negotiations July 2023 | Utilising Data to Understand the Institutions Relationship with a Publisher March 2024\n\nData Analyst at the University of Cambridge Library Niamh Malin, was responsible for supporting the many publisher negotiations. The first paper addresses the use of Microsoft Excel and Dimensions publications data to document the relationship of the university with Springer Nature. The second presentation documents the transition to Microsoft Power BI to create data dashboards which ensure every publisher negotiation is informed by usage and publications data.\n\nUsing Analytics to Extract Value from the Library’s Data - Event Part One: Analytics Behind the Scenes and Part Two: Actionable Data Analysis September 2018.\n\nThe National Information Standards Organization (NISO) hosted a two part webinar in 2018 about extracting value from library data. The slides for the six presentations are available, and cover topics such as; setting up a data analysis strategy, analysing metadata, utilising data visualisations, actioning data insights for utilising physical spaces and building confidence in your data analysis skills.\n\nLibraries support data-driven decision making (February 2024). An OCLC-Liber blog post following their “Building for the future” program where librarians had gathered to discuss their understanding and practical interpretations of data driven libraries.\nUtilising new modes of data to enhance research strategy and collaboration (November 2023). Digital science looks at how institutions can get high-quality insights into research by standardising their data systems.",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "workingwdata.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "Working with Data",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\nNow that you’ve found some data, and/or you’ve formulated some questions to answer, how do you know the data will be good enough? Let’s take some time to assess the data, and ensure it is fit for purpose, because a key analogy in data analysis is;\nGIGO: Garbage in, garbage out!.\nThe process of utilising data is a cycle, a standard process model that describes a common approach to data is CRISP-DM (Cross-industry standard process for data mining). It has six stages, listed below.\n\nThis section will enable you to assess your data, through a variety of leading questions, at each stage of the cycle. How long each phase takes will depend entirely on the project, there is no right or wrong, and it is cyclical, you will return to questions as you develop the project. These are structured as questions because one guide cannot have all the answers, and therefore giving a variety of questions which hopefully ensure you do not miss anything, and learn much more which is relevant to your project. This approach will enable you to assess if the data you have is relevant to the question, and if it has the potential to answer it correctly.\nThis guide hopes to calm your nerves about using data, and to see that you are capable of answering data questions confidently.\n\nBusiness Understanding\nThe Business Understanding phase focuses on understanding the objectives and requirements of the project. Here you are planning your project\n\nWhat questions are you answering?\nWho are your stakeholders?\nWhat are the criteria and limitations to the project?\nWhat is the goal of using this data? Doing this project?\nWhat resources are (or are not) available?\nWhat is the timeline on the project? And the data input specifically?\nWhat stage of the project requires data analysis?\nWhat is the expected outcome of the data analysis?\n\n\n\nData Understanding\nData Understanding drives the focus to identify, collect, and analyse the relevant datasets.\n\nCan you access the data you require? What is the source?\nWhat data (and metadata) is missing?\nHow reliable is the data?\nWhat relationships are relevant within the data?\nWho is involved in gathering and preparing the data?\n\n\n\nData Preparation\nA common rule of thumb is that 80% of the project is data preparation.\n\nWhat data is not necessary from the dataset?\nHow are errors or duplications handled?\nWhat new attributes or formulas are required?\nHow do datasets interact with one another?\nIs the data formatted correctly?\nWhat acronyms are in use? Are they formatted correctly?\nHas the data been standardised?\nAre the column names useful and appropriate?\nDoes every database have a unique identifier column?\nHow do you handle missing metadata?\nAre you editing the master / only copy?\nHave you documented the process so that you can provide evidence if needed?\nIs the data in a clear and useable tabular format (rows and columns)?\nHave all merged cells been removed / updated?\n\n\n\nModelling\nWhat is often regarded as the most exciting work is also often the shortest phase. Now is the time to build and assess various models or visualisations.\n\nWhat conclusions can be drawn from the data?\nWhat visualisations are appropriate to display the data?\nWhat comparisons and relationships should be highlighted to align with your initial goal?\nDoes the data contradict the hypothesis? Why?\nDo the formulas need to be live? Tip: Live formula within a spreadsheet can cause it to be slow and large in size.\n\n\n\nEvaluation\nThis phase looks more broadly at the data project and what to do next.\n\nDid the data fulfil the goal of this project?\nWhat was unable to be achieved?\nWhat is required for the next data project to run more efficiently?\nHas the project been completed successfully?\n\n\n\nDeployment\nDepending on the requirements, the deployment phase can be as simple as emailing a graph, or as complex as publishing a live dashboard of intricate data and visuals.\n\nWho requires the outcome of this project?\nWill the outcome need to be presented in multiple formats?\nWill this be repeated again? Has it been documented?\nWhere will the data and outcome be stored?\n\n\nStoring a datafile / spreadsheet\nOnce the data exists, it must be accessible. There are a variety of software which can store your data, and taking the time to assess the needs of your data is important.\n\nIs the file formatted correctly (CSV vs Microsoft Excel)?\nWill it need to be accessed online and/or offline?\nDoes the file require a software licence to access?\nWhat backups are required?\nHow long are files required to be stored?",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#recommended-readingviewing",
    "href": "workingwdata.html#recommended-readingviewing",
    "title": "Working with Data",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\nDefining Data Librarians\n\nDefining data librarianship: a survey of competencies, skills, and training (July 2018). Federer aims to define data librarianship by exploring the skills and knowledge that data librarians utilise and the training that they need to succeed.\nIntroduction to Databrarianship: The Academic Data Librarian in Theory and Practice (2016). Thompson and Kellam explore the diverse field of data librarianship, highlighting its key commitment to accessible data.\nData librarianship: a day in the life (2011). Interviews with data librarians across the world highlight the challenge and opportunities of data in libraries, including the creation of data services within an academic library.\nThe future for numeric data services (2011). Exploring the future of data librarians, with trends in visualisation, mapping, standardisation, citation and data management plans.\nLibrarian roles in the digital data-driven world (September 2021). Thai/English paper exploring the role of data librarians as someone who continues to acquire new knowledge and skills with technology and the digital era.\nCILIP (Chartered Institute of Library and Information Professionals) provides definitions on the data science roles available within a library.\n\n\n\nLibrary Endorsed Resources\n\nLIBER members have created the Digital Scholarship & Data Science Essentials for Library Professionals, which this guide is a part of. Guidance also includes data visualisation and data as collections, and continues to grow.\nJISC has published resources to support data-driven decisions, including interactive insights on graduate outcomes and conducting online surveys as a form of data gathering.\nThe DSVIL (Data Science and Visualization Institute for Librarians) provides great guidance and resources for finding, cleaning, analysing, visualising and managing data.\nThe Bodleian Library, University of Oxford, hosts a variety of resources to support data analysis (including audio-visual), data mining, and visualisation tools.\nDuke University Libraries have a plethora of resources for data science, data management and data visualisation freely available.\nARL (Associations of Research Libraries) demonstrate the impact data analytics can have within libraries and library communities.\nIFLA (International Federation of Library Associations and Institutions) has a variety of data analytics resources which cover using specific software, trends in the news, and practical applications of data within libraries.\nThe PLA (Public Library Association) (which is part of ALA, the American Library Association), have developed data tools to enable public libraries to be compared across multiple metrics.\n\n\n\nLearning Data Beyond Libraries\nThere are numerous platforms dedicated to data skills, listed below are some of the most popular for beginning your data journey.\n\nThe Carpentries: Community-led coding and data science courses for researchers and librarians, with three specialties; Data Carpentry (data skills for conducting research), Library Carpentry (software and data skills for librarians), and Software Carpentry (lab skills for research computing).\nDatacamp: great for free cheat sheets (webpage or PDF) on data literacy, understanding data knowledge levels and data storytelling. As well as resources and courses for a variety of coding and data software.\nLinkedIn Learning: a wonderful resource of videos for anything from coding to people management to making the perfect Microsoft Excel graph.\nGoogle Skillshop: Training and certification in google analytics which could upport queries about library engagement.",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "workingwdata.html#taking-the-next-step",
    "href": "workingwdata.html#taking-the-next-step",
    "title": "Working with Data",
    "section": "Taking the next step",
    "text": "Taking the next step\n\nEnsuring Good Practice\nWorking with data is very exciting and rewarding, but can quickly become overwhelming. There are fears of being hacked and having data stolen, or even losing data due to bad organisation or not knowing it needed saving. Sadly this guide cannot have all of the answers on the practices and policies which should be in place. But it will instead ask a few provoking questions to ensure you can rest the worries of data loss and theft.\n\nHave you established a naming convention for files? Does it account for versions, dating and author? Tip: Have titles be meaningful, in a relevant order, and versions discernible.\nWhen will the file(s) be deleted or archived?\nIs there clear documentation of the processes undertaken? Therefore ensuring the task can be repeated, or to justify any conclusions. Tip: This should include the source of the data, any cleaning steps made, and all relevant metadata.\nWhere are the files stored, are the folders easy to access? Is there a naming convention and folder hierarchy in place for the project, and the team?\n\n\n\nWhat skills are you looking for?\nIf you have read through this guide, and still thirst for more, here are some questions to encourage your exploration in library data!\n\nWhat in-house data can you combine (such as the library management system and google analytics)?\nThink of a report that is repeated regularly but often time consuming, is there a software or data task that could make this quicker?\nWhat software do you regularly use, how could you expand those skills? Does the library management system provide specialist training?\nIs coding, AI or visualisation an area you would like to explore? What else do you want to do with data?\nWhich elements of the data process do you want to focus? Cleaning, analysing, visualising etc.\nCan you attend a conference which discusses library data you are interested in?",
    "crumbs": [
      "Working with Data"
    ]
  },
  {
    "objectID": "dstp.html",
    "href": "dstp.html",
    "title": "Start your own local training programme",
    "section": "",
    "text": "Find your people\nGet a small group of willing colleagues at your institution together and set up a way for them to communicate regularly, preferably asynchronously/remotely. We use a corporate wide MS Teams channel for this but you could also set up things like a slack channel or even a whatsapp channel. Focus on your purpose of bringing people together, think about the messaging and keep it light, “learn new stuff together” usually suffices! Try not to be too targeted, cast the net wide as people of all interests/academic disciplines/job profiles, abilities and backgrounds will undoubtedly have something to contribute: digital scholarship and data science is a collaborative affair and your group will be all the better for its diversity.",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "dstp.html#identify-shared-needs",
    "href": "dstp.html#identify-shared-needs",
    "title": "Start your own local training programme",
    "section": "Identify shared needs",
    "text": "Identify shared needs\nWhat are problems colleagues at your institution are facing (cataloguing backlogs? etc), and what computational methods might help them to solve these? By framing digital scholarship/data science skill training as an investment that can be a helpful tool for colleagues to resolve long standing curation, creation you will find a more willing audience for your efforts. Quick wins in the digital realm (for instance in learning a tool like Open Refine to normalise and analyse catalogue records) can often build confidence very quickly and open the door to trying out new technologies.",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "dstp.html#get-buy-in-if-you-need-to",
    "href": "dstp.html#get-buy-in-if-you-need-to",
    "title": "Start your own local training programme",
    "section": "Get Buy-in (if you need to!)",
    "text": "Get Buy-in (if you need to!)\nIn some cases you may need buy-in from managers to allow staff time and space to dedicate to learning these new skills. It can be difficult to convince managers sometimes of the value of learning skills now which may take some years to truly embed or come to practical fruition. Digital Scholarship & Data Science Essentials contains loads of evidence to share with your management for why investing in digital scholarship and data science staff skills is key to sustainable digital transformation over time. Have a look at the Skills Competency Frameworks & Key Reports sections for the latest supporting research on this. Each Topic Guide also contains useful real-life applications of new technologies being put into action under the Relevance to the Library Sector (Case Studies/Use Cases). At the British Library we’ve started to capture individual staff digital transformation stories as well which help to demonstrate the tangible value to individuals and institutions.\n\n“The Digital Scholarship Training Programme has introduced me to new software, opened my eyes to digital opportunities, provided inspiration for me to improve, and helped me attain new skills” -Graham Jevon, British Library\n\nRead more about Graham Jevon’s digital transformation journey on the British Library Digital Scholarship blog or have a look at this series of videos we created to mark the 10th Anniversary of the British Library Digital Scholarship Training Programme in 2022.\n\n\n\nHow are you using your new skills in your own work?",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "dstp.html#run-a-monthly-hack-yack",
    "href": "dstp.html#run-a-monthly-hack-yack",
    "title": "Start your own local training programme",
    "section": "Run a monthly Hack & Yack",
    "text": "Run a monthly Hack & Yack\nEstablishing a regular Hack & Yack meeting is a really nice way to learn something new with other colleagues. At the British Library our Hack & Yack’s are once a month for two hours on a set day and time. It is a casual, hands-on session where colleagues from across all departments of the institution come together to understand a current topic or digital method and work through an online tutorial at everyone’s own pace but with support of colleagues.\nWe use it as an opportunity to explore new tools/techniques/applications relevant to digital research and keep our own skills up to speed. Note this is not a formal training session, we’re all learning together and come to the tutorials with a variety of experience/knowledge/skills.\nEach Hack & Yack usually starts with one person, which may or may not be the organiser, giving a high-level view of the topic of the day, and then sharing one or more tutorials that the group can try out, either stepping through one online activity together as a group or providing time for everyone to explore individually at their own pace and chat about how it’s going as they work through the steps. These sessions aren’t recorded so that attendees can be open and frank about their experience!\nWithin each of these Topic Guides is a Hands-on activity and other self-guided tutorial(s) section that would make a perfect start for your first Hack & Yacks!",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "dstp.html#start-a-discussion-group",
    "href": "dstp.html#start-a-discussion-group",
    "title": "Start your own local training programme",
    "section": "Start a discussion group",
    "text": "Start a discussion group\nWhether you call it a ‘reading group’, a discussion group, a lunchtime series - the point is to provide regular opportunities for people to get together, and learn and support each other through discussion. You might set an article, blog post or chapter, a video or podcast.\nThings we’ve found useful:\nBegin each session with quick introductions - name and department, or name and another useful piece of information. Why? Hopefully you’ll have a range of folk and they mightn’t all have met before, and it means that everyone has spoken at least once right from the start.\nMake it ok for people not to have finished reading, watching or listening to the thing you’re discussing. You could do a show of hands to see who’s finished it or not finished it. Why? There’s something about ‘confessing’ that lets people ask questions without worrying that it was covered somewhere towards the end. It can also be useful to understand why people didn’t finish (unless they just ran out of time, which is highly relatable) - did the piece get complex, jargony, boring?\nEach Topic Guide has a Recommended Reading/Viewing section and you can also find some good starting points under Recommended Reading Lists.",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "dstp.html#create-an-identity",
    "href": "dstp.html#create-an-identity",
    "title": "Start your own local training programme",
    "section": "Create an identity",
    "text": "Create an identity\nAfter your group of the willing has been running for awhile it’s useful to create an identify for yourselves. Perhaps you consider yourselves a network, or an interest group or maybe you’d like to start formalising your gatherings under a “training programme” moniker. Whatever you decide, it helps to bring in new folks when there is an established identity and even maybe a logo in advertisements.",
    "crumbs": [
      "Start your own local training programme"
    ]
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "What is an API?",
    "section": "",
    "text": "Introduction\nAny time you write code you are using an API! When you write print(“Hello world”) in Python, the Python Standard Library API is the contract that guarantees the words Hello world will appear on the screen. When you do the same in R, it’s the API of R’s base package that you are using. Other languages’ APIs use other words for the same function (printf, echo, format are some common ones) and may have subtly different interpretations.\nIn many ways an API is like a restaurant menu: the menu sets out the acceptable ways of interacting with the kitchen by specifying what dishes you can order with a brief description of what to expect from the dish, and how much you will have to pay. Items on a menu don’t include a detailed description of their implementation (a recipe), only enough information for the diner to make an informed choice about what they want to eat. If you’re confident you know what you’re doing you can order off-menu, but the results aren’t guaranteed to be what you expect, because there is no documentation of those expectations for the kitchen to refer to.\nAPI is an acronym for Application Programming Interface. Where a GUI (Graphical User Interface) provides a consistent way for humans to interact with an application, an API provides a consistent way for code to interact with other code. APIs set clear expectations that are necessary for\nGenerally “API” can be applied to any consistent interface provided by one computer program for other programs to use, but the most common you are likely to encounter in Digital Scholarship work are:\nBoth of these examples share some important elements. They specify:\nLet’s look at those two types in more detail through this lens.",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#introduction",
    "href": "api.html#introduction",
    "title": "What is an API?",
    "section": "",
    "text": "A set of functions/variables available through a code library, and their expected behaviour\n\nE.g. the Python Standard Library’s API includes a module called re which provides a number of functions, classes and constants that allow you to work with regular expressions\n\nA set of HTTP URLs provided by a system/service, and the expected behaviour when those URLs are requested from the server by another program\n\nE.g. the DataCite API defines a URL https://api.datacite.org/dois which returns a list of all published DataCite DOIs to the requester, formatted as JSON, and when followed by the parameter ?prefix=10.23636 will filter the returned list to only include DOIs with that prefix\n\n\n\n\nWhat the programmer must do\nWhat response the programmer can expect and in what format\nWhat side-effects the programmer should expect\n\n\n\nExample 1: a Python library API\nThe re module mentioned above includes a function called search with the following documentation:\n\nre.search(pattern, string, flags=0)\nScan through string looking for the first location where the regular expression pattern produces a match, and return a corresponding Match. Return None if no position in the string matches the pattern; note that this is different from finding a zero-length match at some point in the string.\n\n\n\n\n\n\n\nTodo\n\n\n\n\nBreak down this definition into how it describes different parts of how the programmer can use this function\nMaybe move this section to the hands-on activity?\n\n\n\n\n\nExample 2: a HTTP API\n\n\n\n\n\n\nTodo\n\n\n\n\nWorked example of a HTTP call (e.g. the DataCite endpoint mentioned above)\nA diagram will be very helpful!\nShould this include example Python code or is that too much detail?",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#glossary",
    "href": "api.html#glossary",
    "title": "What is an API?",
    "section": "Glossary",
    "text": "Glossary\n\nArgument\nCall\nClient\nHTTP\nLibrary\nParameter\nRequest\nReturn\nServer",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "href": "api.html#relevance-to-the-library-sector-case-studiesuse-cases",
    "title": "What is an API?",
    "section": "Relevance to the Library Sector (Case Studies/Use Cases)",
    "text": "Relevance to the Library Sector (Case Studies/Use Cases)\n\nMany (most?) systems that we use every day have APIs. Knowing what these look like and how to use them in general is an important first step in being able to write software that accesses those systems to do much more than you could do with your own code alone. This becomes even more powerful when you can write “glue” code that brings together parts of two or more APIs",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "href": "api.html#hands-on-activity-and-other-self-guided-tutorials",
    "title": "What is an API?",
    "section": "Hands-on activity and other self-guided tutorial(s)",
    "text": "Hands-on activity and other self-guided tutorial(s)\n\n\nSomething involving Postman?\nExploring published API documentation? E.g. DataCite API docs allow you to run test queries from the web page",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#recommended-readingviewing",
    "href": "api.html#recommended-readingviewing",
    "title": "What is an API?",
    "section": "Recommended Reading/Viewing",
    "text": "Recommended Reading/Viewing\n\n\nWikipedia’s “API” entry is a good jumping off point, including some history\n“HTTP: Learn your browser’s language!”: excellent zine by software engineer Julia Evans (costs 12 USD)",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "api.html#taking-the-next-step",
    "href": "api.html#taking-the-next-step",
    "title": "What is an API?",
    "section": "Taking the next step",
    "text": "Taking the next step",
    "crumbs": [
      "What is an API?"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Recommended Reading Lists",
    "section": "",
    "text": "There is no shortage of recommended reading lists out there, here we try and highlight a few of the most up to date and useful lists out there for the sector today!\nZotero Library for Digital Scholarship and Data Science Essentials for Library Professionals\nThis library contains links to all of the resources referenced within Topic Guides and across our site.\nZotero | Groups &gt; collections as data - projects, initiatives, readings, tools, datasets\nOngoing collection of projects, readings, initiatives, tools, and datasets that are in some way or another related to collections as data. This group is an open resource, welcoming contributions from anyone who has a resource to share.\n[TODO] Data Science in Libraries, is there a Zotero?",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Recommended Reading Lists"
    ]
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "Skills Competency Frameworks & Key Reports",
    "section": "",
    "text": "The following are Skills Competency Frameworks & Key Reports relating to supporting Digital Scholarship and Data Science for library staff\n\nLIBER Publications\n\nLIBER Digital Skills for Library Staff & Researchers Working Group - LIBER Europe have lots of resources here, including a very useful diagram Identifying Open Science Skills for Library Staff & Researchers\nLIBER Job Description Repository Contains job description examples for Digital Curator and other digital roles which reference the types of skills required for such work.\nEurope’s Digital Humanities Landscape: A Study From LIBER’s Digital Humanities & Digital Cultural Heritage Working Group is a report based on a Europe-wide survey run by LIBER’s Digital Humanities & Digital Cultural Heritage Working Group. The survey focused on digital collections and the activities libraries undertake around them. It covered the following topics and themes including staffing/skills\n\n\n\nKey Publications specific to digital scholarship and data science skills for research library staff\n\nThe British Library and the Arts and Humanities Research Council published a report on skills: Scoping Skills and Developing Training Programme for Managing Repository Services in Cultural Heritage Organisations. There is a very useful section (Section 3.) that references several other digital skills frameworks for research library staff across Europe.\nLippincott, Joan K. Directions in Digital Scholarship: Support for Digital, Data-Intensive, and Computational Research in Academic Libraries. Coalition for Networked Information, June 2023. https://doi.org/10.56561/ULHJ1168\nPadilla, Thomas. ‘Responsible Operations: Data Science, Machine Learning, and AI in Libraries’. OCLC, 26 August 2020. https://www.oclc.org/research/publications/2019/oclcresearch-responsible-operations-data-science-machine-learning-ai.html.\nCordell, R. C. (2020). Machine Learning + Libraries: A Report on the State of the Field. LC Labs, Library of Congress. https://labs.loc.gov/static/labs/work/reports/Cordell-LOC-ML-report.pdf\nFederer L. Defining data librarianship: a survey of competencies, skills, and training. J Med Libr Assoc. 2018 Jul;106(3):294-303. doi: 10.5195/jmla.2018.306. Epub 2018 Jul 1. PMID: 29962907; PMCID: PMC6013124.\n\n\n\nGeneral Competencies for Librarians which include reference to digital\n\nAmerican Library Association (ALA) Library Competencies (Various roles): Library Competencies | Tools, Publications & Resources (ala.org) (USA)\nCanadian Association of Research Libraries Competencies for Librarians in Canadian Research Libraries Publications and Documents (including specifically Competencies-Final-EN-1-2.pdf (Canada)\nCILIP: the library and information association Professional Knowledge & Skills Base - (UK)",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Skills Competency Frameworks & Key Reports"
    ]
  },
  {
    "objectID": "training-platforms.html",
    "href": "training-platforms.html",
    "title": "Training Platforms",
    "section": "",
    "text": "When you’re ready to go further and have a better idea of the specific skills you need for a particular task, we can recommend having a good search through these excellent platforms which host a great many in-depth training materials:\n\nDARIAH-Campus\nDARIAH is a pan-European infrastructure for arts and humanities scholars working with computational methods. It supports digital research as well as the teaching of digital research methods. Though not specific to the library professional context, tutorials here are useful for applying techniques to digital collections. https://campus.dariah.eu/\n\n\nThe Glam Workbench\nThe GLAM Workbench is the brainchild of Tim Sherratt, a historian, and is a collection of Jupyter notebooks to help you explore and use data from GLAM institutions (galleries, libraries, archives, and museums). It includes tools, tutorials, examples, hacks, and even some pre-harvested datasets. It’s aimed at researchers in the humanities but has useful tutorials for anyone interested in working with GLAM data. https://glam-workbench.net/\n\n\nIneo\nIneo is a project developed and maintained by CLARIAH that lets you search, browse, find and select digital resources for research in humanities and social sciences. At the end of 2024 it will offer access to thousands of tools, datasets, workflows, standards and learning material. It is a work in progress so do keep that in mind when browsing. https://www.ineo.tools/\n\n\nLibrary Carpentry\nLibrary Carpentry is an international volunteer community, under the Carpentries, focussed building software and data skills within library and information-related communities. The lessons here are meant to be taught as workshops led by a Carpentries certified instructor (for a fee) but you may find it useful to have a read through the content which is open and available to all. https://librarycarpentry.org/\n\n\nThe Programming Historian\nThe Programming Historian has been publishing peer-reviewed tutorials on digital tools and techniques for humanists since 2008 and though they’re generally aimed at academic researchers, staff at British Library have found them highly useful over the years in their own work! https://programminghistorian.org/en/",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Training Platforms"
    ]
  },
  {
    "objectID": "networks.html",
    "href": "networks.html",
    "title": "Useful Networks",
    "section": "",
    "text": "LIBER Working Groups\nWorking groups are open to staff at participating LIBER Member institutions: - LIBER Data Science in Libraries - LIBER Digital Scholarship & Digital Cultural Heritage - Or have a look at the other LIBER Working Groups - LIBER Europe",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  },
  {
    "objectID": "networks.html#international-networks",
    "href": "networks.html#international-networks",
    "title": "Useful Networks",
    "section": "International Networks",
    "text": "International Networks\n\nAI4LAM\nCode4Lib\nElectronic Literature Organisation\nFlickr Commons\nGlam Labs International\nIIIF/UV Open Collective\nIMPACT Centre of Competence\nMuseums Computer Group\nTranskribus\nWikidata Community)",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  },
  {
    "objectID": "networks.html#national-networks-european",
    "href": "networks.html#national-networks-european",
    "title": "Useful Networks",
    "section": "National Networks (European)",
    "text": "National Networks (European)\n\nIreland/UK\n\nRLUK Digital Scholarship Network",
    "crumbs": [
      "**GENERAL RESOURCES**",
      "Useful Networks"
    ]
  }
]